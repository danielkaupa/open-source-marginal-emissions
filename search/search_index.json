{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"A Public-Data Framework for Marginal Emissions Factor Estimation","text":""},{"location":"#repository-overview","title":"Repository Overview","text":"<p>The purpose of this project is to develop an open-source framework that transforms publicly available weather and grid data into directionally accurate marginal emissions factors (MEFs).</p> <p>This project is currently under development, and this repository will be updated as new code is developed.</p> <p>The status of the various modules can be found in the Repository Structure and Contents section below.</p> <p></p>"},{"location":"#project-context-and-overview","title":"Project Context and Overview","text":"<p>Marginal emissions are an important concept when evaluating the carbon impact of electricity usage, and they differ from the better known and more widely used average emissions. Average emissions tell you the carbon intensity of all electricity generated over some period, while marginal emissions tell you the carbon intensity of the next unit of electricity (or the last unit avoided). When evaluating the specific impacts of electricity usage in a given time and location, marginal emissions become much more relevant.</p> <p>In order to truly calculate marginal emissions factors, one needs access to detailed grid data traditionally only held by grid operators. However, there is a growing body of research exploring the possibility of estimating marginal emissions factors using publicly available data sources, such as weather data and aggregate grid demand and emissions data.</p> <p></p>"},{"location":"#project-purpose","title":"Project Purpose","text":"<p>The goal of this project is to provide a transparent, reproducible framework for estimating marginal emissions factors (MEFs) using only publicly available data sources. In doing so, this work aims to make meaningful, time- and location-specific carbon insights available in regions where access to grid dispatch data is limited.</p> <p>This framework focuses on two core capabilities: </p> <ol> <li> <p>Data Acquisition and Preparation - A pipeline to retrieve, clean, and integrate large-scale weather and generation/emissions datasets in a consistent, analysis-ready format.</p> </li> <li> <p>Marginal Emissions Estimation - A modeling workflow for estimating MEFs that are:</p> <ul> <li>directionally accurate,</li> <li>stable across seasonal and diurnal conditions, and</li> <li>transparent to inspect and extend.</li> </ul> </li> </ol> <p>While this project currently develops use cases for India, the workflow is intended to be generalizable, enabling users to adapt the approach to other regions with publicly available data.</p> <p>The documentation for this repository contains more information and definitions for the core concepts of energy, emissions, the grid, and techniques used in this project.</p> <p></p>"},{"location":"#repository-structure-and-contents","title":"Repository Structure and Contents","text":"<p>This repository is designed as a modular framework, where each module can evolve or be replaced independently. The top-level modules are packaged for long-term maintainability and distributed development.</p> <p></p>"},{"location":"#module-overview","title":"Module Overview","text":"Module Description Status weather_data_retrieval Fully packaged CLI + batch utilities to download ERA5 and ERA5-Land data from ECMWF CDS. Open-Meteo under development \u2705 Available Now grid_data_retrieval CLI + batch utilities to download demand, generation, and emissions data from public grid sources. \ud83d\udea7 Under Development data_cleaning_and_joining Spatial alignment, temporal resampling, and integration of grid + weather datasets with memory-efficient formats. \ud83d\udea7 Under Development marginal_emissions_modelling Methods for estimating MEFs and evaluating models. Designed to accommodate several modelling approaches. \ud83d\udea7 Under Development"},{"location":"#repository-structure","title":"Repository Structure","text":"<pre><code>.open-source-marginal-emissions/\n    \u251c\u2500\u2500 configs                 # Configuration files for data retrieval and processing\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 grid                    # Grid data retrieval configurations\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 pipelines               # Data processing pipeline configurations\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 weather                 # Weather data retrieval configurations\n    \u2502\n    \u251c\u2500\u2500 information             # Documentation and additional information\n    \u2502\n    \u251c\u2500\u2500 notebooks               # Jupyter notebooks for exploration and analysis\n    \u2502\n    \u251c\u2500\u2500 packages                                # Modular packages for different components of the framework [Run order]\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 data_cleaning_and_joining               # Data cleaning, spatial alignment, and integration [3]\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 grid_data_retrieval                     # Public grid data retrieval utilities [2]\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 marginal_emissions_modelling            # MEF estimation methods and evaluation [4]\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 osme_common                             # Common utilities and functions for the framework [All]\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 weather_data_retrieval                  # ERA5 (from CDS) and Open-Meteo data retrieval [1]\n    \u2502\n    \u2514\u2500\u2500 README.md       # you are here\n</code></pre>"},{"location":"#additional-information","title":"Additional information","text":""},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This work extends an MSc thesis at Imperial College London and benefited from support by collaborators and mentors. See ACKNOWLEDGEMENTS.</p>"},{"location":"#license","title":"License","text":"<p>This project is released under the terms in LICENSE.</p> <p>License: AGPL-3.0-or-later OR Commercial License For commercial use without AGPL obligations, email daniel.kaupa@outlook.com See COMMERCIAL-TERMS and LICENSE.</p>"},{"location":"ACKNOWLEDGEMENTS/","title":"Acknowledgements","text":"<p>This project extends research conducted as part of an MSc thesis at Imperial College London (Environmental Data Science &amp; Machine Learning). Sincere thanks to mentors, collaborators, and supporters for their invaluable contributions and encouragement throughout this work.</p>"},{"location":"ACKNOWLEDGEMENTS/#institutional-support","title":"Institutional support","text":"<ul> <li>Hitachi\u2013Imperial Centre for Decarbonisation and Natural Climate Solutions</li> <li>Imperial College London \u2013 Data Science Institute</li> </ul>"},{"location":"ACKNOWLEDGEMENTS/#individual-contributions","title":"Individual contributions","text":"<ul> <li>Dr Mirabelle Mu\u00fbls, Dr Shefali Khanna, and Dr Gareth Collins \u2014 supervision, feedback, and guidance.</li> <li>Brython Caley-Davies \u2014 assistance with data infrastructure centralisation and preprocessing.</li> <li>Colleagues and reviewers who provided feedback on methods and results.</li> </ul>"},{"location":"ACKNOWLEDGEMENTS/#resources-and-data","title":"Resources and data","text":"<ul> <li>Data provided by TATA Power.</li> <li>High-Performance Computing (HPC) resources provided by Imperial College London.</li> </ul> <p>If you supported this project and would like your contribution described differently, please open a Pull Request or contact the author.</p>"},{"location":"ACKNOWLEDGEMENTS/#references","title":"References","text":"<ol> <li>Aditya Tomar (2024). margE_India.Rmd [R]. Imperial College London. https://github.com/ese-ada-lovelace-2024/irp-dbk24/blob/main/emission_rate_methodologies/marginal%20emissions/margE_India.Rmd</li> <li>Ajay Shankar, A K Saxena, &amp; Taruna Idnani (2025). Roadmap to India\u2019s 2030 Decarbonization Target. The Energy and Resources Institute. https://www.energy-transitions.org/publications/roadmap-to-indias-2030-decarbonization-target/</li> <li>Bentsen, L. \u00d8., Warakagoda, N. D., Stenbro, R., &amp; Engelstad, P. (2023). Spatio-temporal wind speed forecasting using graph networks and novel Transformer architectures. Applied Energy, 333, 120565. https://doi.org/10.1016/j.apenergy.2022.120565</li> <li>Bracht, F., Khanna, S., Martin, R., &amp; Mu\u00fbls, M. (2025). Waiting for carbon reductions \u2014 A smart IV approach.</li> <li>Buchholz, K. (2022, November 9). Infographic: India\u2019s Electricity Mix. Statista Daily Data. https://www.statista.com/chart/27963/india-energy-mix</li> <li>Center for Social and Economic Progress (2020, September 22). The CSEP Electricity and Carbon Tracker. https://carbontracker.in/ (Accessed 14-07-2025)</li> <li>Clean Energy Buyers Institute (2022). Guide to Sourcing Marginal Emissions Factor Data. https://cebi.org/wp-content/uploads/2022/11/Guide-to-Sourcing-Marginal-Emissions-Factor-Data.pdf</li> <li>Cohen, J. (2013). Statistical Power Analysis for the Behavioral Sciences (2nd ed.). Routledge. https://doi.org/10.4324/9780203771587</li> <li>Council on Foreign Relations (2024, December 5). How Electrification Can Reduce Emissions. https://education.cfr.org/learn/reading/electrification-climate-change</li> <li>Dhesi, B. (2023, January 18). How much does it cost to boil a kettle? Hugo. https://hugoenergyapp.co.uk/cost-to-boil-a-kettle/</li> <li>EIA (2023, June 21). As solar capacity grows, duck curves are getting deeper in California. U.S. EIA \u2013 Today in Energy. https://www.eia.gov/todayinenergy/detail.php?id=56880</li> <li>EIA (n.d.). Hourly Electric Grid Monitor: U.S. Electricity Overview. Retrieved 27 Aug 2025 from https://www.eia.gov/electricity/gridmonitor/index.php</li> <li>Elecenergy (2013). Interval Energy Data, or Itemized Energy Data? https://elecenergy.co.uk/assets/white-paper-interval-energy-data-or-itemized-energy-data.pdf</li> <li>Electricity Maps (2025). Carbon Intensity (Version 2025-01-27) [Dataset]. https://github.com/electricitymaps/electricitymaps-contrib/</li> <li>Energy Dashboard (2025, August 12). Live Generation, Demand and Emissions. https://www.energydashboard.co.uk/live</li> <li>Fleschutz, M., Bohlayer, M., Braun, M., &amp; Murphy, M. D. (2022). Demand Response Analysis Framework (DRAF): An Open-Source Multi-Objective Decision Support Tool for Decarbonizing Local Multi-Energy Systems. Sustainability 14(13). https://doi.org/10.3390/su14138025</li> <li>Green, A. (2024, December 5). Average versus Marginal Carbon Emissions: Many professionals are getting this wrong. ADG Efficiency. https://adgefficiency.com/energy-basics-average-vs-marginal-carbon-emissions/</li> <li>Hawkes, A. D. (2010). Estimating marginal CO\u2082 emission rates for national electricity systems. Energy Policy, 38(10), 5977\u20135987. https://doi.org/10.1016/j.enpol.2010.05.053</li> <li>Hersbach, H. et al. (2023). ERA5 hourly data on single levels from 1940 to present. Copernicus C3S Climate Data Store. DOI: 10.24381/cds.adbb2d47 (Accessed 16-07-2025)</li> <li>How Many Watts Does an Electric Kettle Use? (2023, August 2). BLUETTI-PH. https://www.bluettipower.ph/blogs/news/how-many-watts-does-an-electric-kettle-use</li> <li>IEA (2025, February). Emissions \u2013 Electricity 2025 \u2013 Analysis. https://www.iea.org/reports/electricity-2025/emissions</li> <li>IEA (2025, August 13). Real-Time Electricity Tracker \u2013 Data Tools. https://www.iea.org/data-and-statistics/data-tools/real-time-electricity-tracker</li> <li>International Hydropower Association (2025, August 27). Sediment Management Case Studies: India \u2013 Teesta V. https://www.hydropower.org/sediment-management-case-studies/india-teesta-v</li> <li>LotusArise (2024, February 17). Hydroelectric Power Plants in India [UPSC Mapping]. https://lotusarise.com/hydroelectric-power-plants-in-india-upsc/</li> <li>Manembu, P., Kewo, A., Bramstoft, R., &amp; Nielsen, P. S. (2023). A Review of Residential Electricity Load Shifting at the Appliance Level. Engineering. https://doi.org/10.20944/preprints202310.0908.v1</li> <li>Mu\u00f1oz Sabater, J. (2019). ERA5-Land hourly data from 1950 to present. Copernicus C3S Climate Data Store. DOI: 10.24381/cds.e2161bac (Accessed 14-07-2025)</li> <li>Naga Gangadhar (2024, December 12). Top 7 Largest Hydroelectric Power Plants in India (2025). Blackridge Research &amp; Consulting. https://www.blackridgeresearch.com/blog/top-seven-hydroelectric-power-plants-in-india</li> <li>Open-Meteo.com (n.d.). Free Open-Source Weather API. Retrieved 27 Aug 2025 from https://open-meteo.com/</li> <li>Oracle (n.d.). Interval Data File Specifications. Retrieved 27 Aug 2025 from https://docs.oracle.com/en/industries/energy-water/opower-platform/data-transfer/interval_AMI_spec/DataFileSpecifications_AMI_.htm</li> <li>Ritchie, H., Roser, M., &amp; Rosado, P. (2020). Renewable Energy. Our World in Data. https://ourworldindata.org/renewable-energy</li> <li>Roser, M. (2020). Why did renewables become so cheap so fast? Our World in Data. https://ourworldindata.org/cheap-renewables-growth</li> <li>Cunningham, S. (2021). 7 Instrumental Variables. In Causal Inference: The Mixtape. Yale University Press. https://mixtape.scunning.com/07-instrumental_variables</li> <li>Sengupta, S. et al. (2022). Current and Future Estimates of Marginal Emission Factors for Indian Power Generation. Environmental Science &amp; Technology, 56(13), 9237\u20139250. https://doi.org/10.1021/acs.est.1c07500</li> <li>Siler-Evans, K., Azevedo, I. L., &amp; Morgan, M. G. (2012). Marginal Emissions Factors for the U.S. Electricity System. Environmental Science &amp; Technology, 46(9), 4742\u20134748. https://doi.org/10.1021/es300145v</li> <li>statstutor.ac.uk (2011, December 16). Pearson\u2019s correlation. https://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf</li> <li>Singh, V. &amp; Bond, K. (2024). Powering Up the Global South: The Cleantech Path to Growth. Rocky Mountain Institute. https://rmi.org/insight/powering-up-the-global-south/</li> <li>WattTime (2022). Marginal Emissions Modeling: WattTime\u2019s Approach to Modeling and Validation. https://watttime.org/wp-content/uploads/2023/11/WattTime-MOER-modeling-20221004.pdf</li> <li>WattTime (n.d.). Average vs. Marginal Emissions. Retrieved 30 May 2025 from https://watttime.org/data-science/data-signals/average-vs-marginal/</li> </ol>"},{"location":"CORE_CONCEPTS_AND_DEFINITIONS/","title":"Core Concepts and Definitions:","text":"<p>The purpose of this page is to provide definitions and explanations of key terms and concepts related to this project.</p>"},{"location":"CORE_CONCEPTS_AND_DEFINITIONS/#emissions-and-emissions-factors","title":"Emissions and Emissions Factors","text":"Emission factors <ul> <li> <p>Definitions:</p> <ul> <li>\"An emission factor gives the relationship between the amount of a pollutant produced and the amount of raw material processed or burnt. For example, for mobile sources, the emission factor is given in terms of the relationship between the amount of a pollutant that is produced and the number of vehicle miles travelled. By using the emission factor of a pollutant and specific data regarding quantities of materials used by a given source, it is possible to compute emissions for the source. This approach is used in preparing an emissions inventory.\" DEFRA</li> <li>\"An emission factor (EF) is a coefficient that describes the rate at which a given activity releases greenhouse gases (GHGs) into the atmosphere. They are also referred to as conversion factors, emission intensity and carbon intensity.\" Climatiq</li> </ul> </li> <li> <p>Notes: This factor can vary on a number of factors including but not limited to the specific technology used, age and condition of a technology, the specific chemical properties of a given type of fuel. To illustrate the affect of the technology, take for example a coal-fired generator that is 30 years old. This generator will probably be less efficient than one that is 5 years old, and require more fuel to produce the same amount of electricity. More coal consumed in this scenario means more carbon and emissions per kWh. There can also be differences within the same category of fuel. For example bituminous coal generally has a higher heat content than lignite, and the overall footprint of carbon from natural gas may will vary based on its source or mixture</p> </li> </ul> Average Emissions <ul> <li> <p>Definition: The total carbon emissions per amount of electricity produced by all generators in a system. ADG Efficiency</p> </li> <li> <p>Notes: Average emissions are good for analysing environmental impact in the aggregate - over a year or over a large area. Using average emission factors helps simplify the calculations by providing a single value to represent the emissions from a system that contains a diverse set of power generation sources.</p> </li> </ul> Marginal Emissions <ul> <li> <p>Definition: \"Marginal emissions refer to the amount of greenhouse gases or other pollutants emitted per unit of energy generated or consumed by the last power plant brought online or taken offline to meet fluctuating demand.\" Powerledger</p> </li> <li> <p>Notes: To borrow from logistics and transportation - you can think of marginal emissions as analogous to the 'last mile' that your Amazon delivery takes on a shipment. Before getting to its final destination, there were likely a few different cargo ships, maybe rail cars or semi-trucks that carried your package before it was handed off to a local post office to be delivered. The cargo ship, train, and truck were all likely taking those routes regardless of your action to order this package. But the journey to your home to deliver the package was is a result directly caused by your action to order that package. Similarly, marginal emissions are the emissions associated with the last or next unit of electricity generated to meet demand (deliver your package in this analogy).</p> </li> <li> <p>Characteristics of Marginal Emissions:</p> <ul> <li>The marginal generator is likely to be the most expensive generator at that time. (It may not be if the plant needs to be kept on for technical reasons). As renewables are characterized by low marginal costs, they are the ones that are unlikely to be pushed off the grid. ADG Efficiency</li> <li>Additional sources: DESNZ, Electricity Maps, Lane Clark &amp; Peacock LLP</li> </ul> </li> </ul> When to use Average Emissions &amp; Marginal Emissions <ul> <li>Average emissions are useful for understanding the overall impact of a power generation system and for long-term planning and policy-making. They provide a broad view of emissions across all generators in a system.</li> <li>Marginal emissions are more relevant for understanding the impact of specific actions or changes in electricity consumption, such as shifting demand to different times of the day or adding new renewable energy sources. They help identify the emissions associated with the next unit of electricity generated or consumed.</li> <li>\"The scientific consensus is clear: marginal emissions rates are the appropriate metric to use for measuring the consequential impact of load shifting, siting new loads, and building new renewables.\" WattTime</li> </ul>"},{"location":"CORE_CONCEPTS_AND_DEFINITIONS/#energy-and-the-grid","title":"Energy and the Grid","text":"Baseload Power <ul> <li>Definition: The amount of power made available by an energy producer (such as a power plant) to meet fundamental demands by consumers. Merriam-Webster</li> </ul> Demand Response <ul> <li>Definitions:<ul> <li>\"The changes in electric usage by end-use customers from their normal consumption patterns in response to changes in the price of electricity over time.\"</li> <li>\"The incentive payments designed to induce lower electricity use at times of high wholesale market prices or when system reliability is jeopardized.\"</li> </ul> </li> <li>Notes:<ul> <li>It \"includes all intentional modifications to consumption patterns of electricity of end use customers that are intended to alter the timing, level of instantaneous demand, or the total electricity consumption.\"</li> </ul> </li> <li>Source: IEEE</li> </ul> Grid Energy Storage <ul> <li>Definition: The technologies that are connected to the electrical power grid that store energy for later use (e.g. pumped-storage hydroelectricity, Vehicle to Grid, Batteries).</li> <li>Purpose: These systems help balance supply and demand by storing excess electricity from variable renewables such as solar and inflexible sources like nuclear power, releasing it when needed.</li> <li>Source: Wikipedia</li> </ul> Load Balancing <ul> <li>Definition: Load Balancing refers to the processes and techniques used to store extra power during periods with lower demand, and then release that power during periods of high demand. The aim is for the power supply system to have a load factor of 1. Wikipedia</li> <li>For more information on load factors see Electrical Rates: The Load Factor and the Density Factor</li> <li>Also known as load matching or daily peak demand reserve</li> </ul> Locational Marginal Pricing (LMP) <ul> <li>Definition: A pricing mechanism used in electricity markets to determine the cost of delivering electricity at specific locations, taking into account the cost of generation and the constraints of the transmission network.</li> <li>The marginal price of electricity at a specific location (node) in the power grid, reflecting the cost of supplying the next increment of electricity demand at that location, considering generation costs and transmission constraints.</li> <li>Purpose: LMP helps to ensure that electricity prices reflect the true cost of supplying power, including the impact of congestion and losses in the transmission system.</li> <li>Sources: UKERC, ISO-NE, Enverus</li> </ul> Peak Hours <ul> <li>Definition: The hours in a day when people generally consume more electricity due to their daily routine such as getting ready for work in the morning or making dinner in the evening.</li> <li>Notes: Peak hours usually occur in the morning or late afternoon/evening  (depending on location).<ul> <li>In temperate climates - the peak is usually when household appliances are heavily used in the evening after work hours.</li> <li>In hot climates - the peak is usually late afternoon when the AC load is high - also workplaces are still open and consuming power.</li> <li>In cold climates - the peak may be in the morning when the space heating and industry are both starting up.</li> </ul> </li> <li>Sources: Wikipedia, British Gas</li> </ul> Peaking Power Plants <p>Definition: The power plants that only run (or mainly run ) when there is a high demand. Because they only supply power occasionally - their power is usually more expensive per kilowatt hour than the base load.</p> <p>Notes:     - These plants are different than the base load power plants - which are those that supply a dependable and consistent amount of electricity.     - Certain peaker plants may operate a few hours a day, or others only a few hours a year.     - Common peaker plants are  gas turbines or gas engines that burn natural gas - though some burn biogas or other petroleum derivations.         - Their efficiency is generally 20-42% for simple ones, and 30-42% for newer ones.         - The New York Power Authority (NYPA) is looking to replace gas peaker plants with battery storage. Ventura County in California was able to do this with Tesla Megapacks, and in Lessines, Belgium 40 Megapacks replaced a turbojet generator.</p> <p>Source: Wikipedia</p> <p>Also known as peakers or peaker power plants</p>"},{"location":"CORE_CONCEPTS_AND_DEFINITIONS/#economics","title":"Economics","text":"Marginal Cost <p>Definition: The cost of producing one additional unit of a good or service. It is a key concept in economics and is used to analyze the behavior of firms in competitive markets.Investopedia</p> (Average) Treatment effect <ul> <li>Definitions:<ul> <li>The \"average causal effect of a binary variable on an outcome variable of scientific or policy interest.\" MIT</li> <li>A \"measure used to estimate the causal effect of a treatment or intervention on an outcome.\" CausalWizard</li> </ul> </li> </ul>"},{"location":"CORE_CONCEPTS_AND_DEFINITIONS/#data-science-mathematics","title":"Data Science / Mathematics","text":"Greedy Algorithm <p>Definition: A problem-solving heuristic that makes the locally optimal choice at each stage with the hope of finding a global optimum. Geeks for Geeks</p> Ordinary Least Squares Regression (OLS) <p>Definition: A technique for estimating the coefficients of a linear regression equation by minimizing the sum of the squared differences between the observed and predicted values. XLSTAT</p>"},{"location":"PROJECT_ROADMAP/","title":"Project Roadmap","text":"<p>This page tracks the rough priorities for each package in the <code>open-source-marginal-emissions</code> monorepo as well as the overall platform. It summarises the plan for extending the MSc thesis codebase into a modular, reproducible platform that supports multiple data providers, scalable computation, and transparent data-processing pipelines.</p>"},{"location":"PROJECT_ROADMAP/#weather_data_retrieval","title":"<code>weather_data_retrieval</code>","text":"<p>Goal: Automated retrieval of weather datasets from APIs including ERA5 from the Copernicus Climate Data Store and Open-Meteo.</p>"},{"location":"PROJECT_ROADMAP/#mvp-deliverable","title":"MVP Deliverable","text":"<ul> <li>Reproducible retrieval of ERA5-world and ERA5-land data from the Copernicus CDS API for user-defined regions and date ranges.</li> <li>Allows for both interactive CLI and batch JSON config usage.</li> </ul>"},{"location":"PROJECT_ROADMAP/#future-development","title":"Future Development","text":"<ul> <li>Implement Open-Meteo data retrieval.</li> <li>Increase robustness of dataset validation for variable availability.</li> <li>Implement functionality that allows users to view the data variables available for their selected dataset prior to selection for download.</li> <li>Improve logging output and consistency between interactive and batch modes.</li> <li>Increase robustness of estimated file size calculations prior to download.</li> <li>Add functionality for further splitting data into larger or smaller chunks based on user preference.</li> <li>Specifically with the goal of having some files 'github compatible' (&lt;100MB).</li> <li>Ensure that the \u201cYou already have this dataset downloaded, are you sure you want to proceed with the new download?\u201d functionality is foolproof / optimised.</li> </ul>"},{"location":"PROJECT_ROADMAP/#grid_data_retrieval","title":"<code>grid_data_retrieval</code>","text":"<p>Goal: Automated retrieval of electricity grid datasets from APIs including Electricity Maps, International Energy Agency, and country-specific sources such as CarbonTracker India.</p>"},{"location":"PROJECT_ROADMAP/#mvp-deliverable_1","title":"MVP Deliverable","text":"<ul> <li>Reproducible retrieval of electricity grid data from carbontracker.in for user-defined date ranges.</li> <li>Allows for both interactive CLI and batch JSON config usage.</li> <li>Attempt to use existing library for CLI (different strategy from weather_data_retrieval)</li> </ul>"},{"location":"PROJECT_ROADMAP/#future-development_1","title":"Future Development","text":"<ul> <li>Implement Electricity Maps data retrieval.</li> <li>Implement IEA data retrieval.</li> <li>Implement country-specific data retrieval for additional countries beyond India.<ul> <li>Desired test regions: Argentina, Greece, South Africa, Costa Rica, Panama.</li> </ul> </li> <li>Increase robustness of dataset validation for variable availability.</li> <li>Implement functionality that allows users to view the data variables available for their selected dataset prior to selection for download.</li> <li>Improve logging output and consistency between interactive and batch modes.</li> <li>Increase robustness of estimated file size calculations prior to download.</li> <li>Add functionality for further splitting data into larger or smaller chunks based on user preference.</li> <li>Specifically with the goal of having some files 'github compatible' (&lt;100MB).</li> <li>Ensure that the \u201cYou already have this dataset downloaded, are you sure you want to proceed with the new download?\u201d functionality is foolproof / optimised.</li> </ul>"},{"location":"PROJECT_ROADMAP/#data_cleaning_and_joining","title":"<code>data_cleaning_and_joining</code>","text":"<p>Goal: Automated cleaning of raw weather and grid datasets including: * Profiling and filling gaps where needed. * Aligning datasets to common temporal frequencies (e.g., 30-minutely). * De-accumulating weather variables where necessary (e.g., ERA5 precipitation). * Aggregating or disaggregating grid data to common temporal frequencies. * Joining weather and grid datasets on common axes (time, region).</p>"},{"location":"PROJECT_ROADMAP/#mvp-deliverable_2","title":"MVP Deliverable","text":"<ul> <li>Process weather and grid data to export analysis-ready tables at 30-minutely frequency.<ul> <li>Includes gap filling, aligning of geospatial/temporal axes, de-accumulation of weather data, and aggregation/disaggregation of grid data.</li> </ul> </li> <li>Include national average of weather data as well as data at native resolution.</li> <li>(Trim data to national boundaries - India)</li> </ul>"},{"location":"PROJECT_ROADMAP/#future-development_2","title":"Future Development","text":"<ul> <li>Select and trim data to national boundaries</li> <li>Allow for configurable temporal frequencies (60, 120 first - maybe 15 as final)</li> <li>Allow for configurable geospatial resolutions (regrid to coarser (aggregate) or finer (disaggregate) grids)</li> <li>Allow for export options of one master file, split files by year, size aware chunking (and github compatible files &lt;100MB)</li> <li>Implement CLI and batch JSON config for data cleaning and joining.</li> <li>Allow for user-defined gap filling strategies when completeness is below a certain threshold.</li> </ul>"},{"location":"PROJECT_ROADMAP/#marginal_emissions_modelling","title":"<code>marginal_emissions_modelling</code>","text":"<p>Goal: Allow users to develop a model for marginal emissions factors using the weather and grid data available</p>"},{"location":"PROJECT_ROADMAP/#mvp-deliverable_3","title":"MVP Deliverable","text":"<ul> <li>Notebook style development with one for EDA and one for generating models</li> <li>Template for how to sift through data, transformations, models, and evaluation of accuracy</li> <li>Expected end results and format: datetime, MEF, AEF</li> </ul>"},{"location":"PROJECT_ROADMAP/#future-development_3","title":"Future Development","text":"<ul> <li>Show how to incorporate additional data beyond the weather and grid information:<ul> <li>For example, it may be interesting to do weightings for the weather data based on the region it originates from.</li> <li>This could be useful in terms of tying the weights to variables such as percentage of national electricity generation or consumption, sources of electricity generation, sources of electricity consumption (industry vs residential), population, living quality indices, or other.</li> </ul> </li> <li>This of course brings up related questions of how does this additional data tie into the existing data we have and relate to it?<ul> <li>For example \u2013 if there is more solar production in one region \u2013 should we weight the solar radiation variables more heavily? And if so \u2013 how do we ensure that we weight it in a way that it doesn\u2019t discount the others? Similar questions can be asked of precipitation and hydro power.</li> </ul> </li> <li>Tie model runs to weights and biases to allow for comprehensive model and results evaluation</li> <li>Programmatically run multiple models for a basic set of variables and predefined transformations.<ul> <li>User would choose<ul> <li>Which models to be included / run</li> <li>Which variables to use (predefined transformations)</li> <li>Hyperparameters would revert to their defaults</li> <li>The best result(s) would be returned to the user.</li> </ul> </li> </ul> </li> <li>Programmatically run all combinations of predefined variable and hyperparameter sets for a given model<ul> <li>User would choose:<ul> <li>1 model to be run</li> <li>Set of variables to use and test combinations of (predefined transformations)</li> <li>Set of hyperparameters to use and test combinations of</li> </ul> </li> <li>Would show the user all of the different results.</li> </ul> </li> <li>Incorporate ChatGPT or other tool to be able to look into your code and recommend what the next steps would be / what model they would recommend.</li> <li>Provide a few different potential transformations \u2013 or links to resources for where to learn about which ones would be appropriate under these circumstances.<ul> <li>Pursue data transformations (replacing originals)</li> </ul> </li> </ul>"},{"location":"PROJECT_ROADMAP/#residential_electricity_data_retrieval","title":"<code>residential_electricity_data_retrieval</code>","text":"<p>Goal: Automated retrieval of open-source residential electricity consumption datasets from APIs. Potential sources include Energy Dashboard UK and country-specific sources.</p>"},{"location":"PROJECT_ROADMAP/#mvp-deliverable_4","title":"MVP Deliverable","text":"<ul> <li>Reproducible retrieval of residential electricity consumption data from any open-source API.</li> </ul>"},{"location":"PROJECT_ROADMAP/#future-development_4","title":"Future Development","text":"<ul> <li>CLI interface for user interaction showing available datasets, variables, date ranges.</li> <li>Batch JSON config for automated retrieval.</li> <li>Data validation and completeness checks.</li> <li>Gap filling strategies for missing data.</li> </ul>"},{"location":"PROJECT_ROADMAP/#optimisation","title":"<code>optimisation</code>","text":"<p>Goal: Module for running optimisation processes that determine the most efficient scheduling or allocation of electricity demand, based on marginal emissions and operational constraints.</p>"},{"location":"PROJECT_ROADMAP/#mvp-deliverable_5","title":"MVP Deliverable","text":"<ul> <li>Module that performs optimisation for a predefined set of constraints and time periods (e.g., weekly or monthly).</li> <li>Accepts and validates all required inputs (constraints, data sources, solver selection).</li> <li>Produces a reproducible output containing the optimised schedule or decision variables, as well as summary statistics and diagnostics.</li> <li>Support one solver (greedy or MILP).</li> </ul>"},{"location":"PROJECT_ROADMAP/#future-development_5","title":"Future Development","text":"<ul> <li>Implement new solvers with predefined constraints<ul> <li>Greedy, MILP, continuous solvers, and potentially heuristic/metaheuristic algorithms.</li> <li>Automated logging of optimisation runs, constraints used, and computational statistics to allow for comparison of solver performance across constraints and datasets.</li> </ul> </li> <li>Allow for user-defined constraints and solvers<ul> <li>Enhanced modularity to allow plug-and-play functionality for new solvers or constraint sets without affecting the broader pipeline</li> <li>Must handle both continuous and mixed-integer constraint definitions.</li> <li>Implement clear error handling and validation if incompatible constraints or solvers are chosen.</li> </ul> </li> <li>Enables modular inclusion of input metrics and variables.</li> <li>Provides scripts to flexibly pre-compute necessary inputs based on user input (e.g., city-week shards, household floors, regional demand factors).</li> <li>Validates that required inputs exist before running optimisation.</li> <li>Needed because these may require significant processing to determine</li> <li>Allow user to select execution mode:</li> <li>Local (small dataset or short time horizon such as a week or month).</li> <li>HPC (large-scale or full-period optimisation).</li> <li>Provides estimates of required computational resources (RAM, runtime) before execution, based on dataset size and solver selection.</li> <li>Supports efficient data handling through Polars or other lazy computation backends (potential C++ integration if performance critical).</li> <li>Allows user to define optimisation periods (e.g., week, month) and perform proportional scaling for data that spans larger intervals (e.g., city/quarter).</li> <li>Web-based form or graphical interface for defining optimisation constraints, solver selection, and execution parameters (building upon the command-line interface).</li> <li>Implementation should include memory warnings and dynamic scaling for dataset size.</li> <li>Develop a comprehensive reference list of all possible constraint and variable types, with example definitions and sample input files.</li> <li>Potential future requirement: automatic retrieval of regional electricity usage or consumption profiles to feed regional-level optimisation scenarios.</li> </ul>"},{"location":"PROJECT_ROADMAP/#overall-platform","title":"Overall Platform","text":"<p>Goal: To provide a unified, user-friendly entry point and a robust technical foundation for the entire open-source-marginal-emissions platform. This involves creating intuitive user interfaces, seamless workflows, and shared infrastructure that enables modularity, reproducibility, and scalability.</p>"},{"location":"PROJECT_ROADMAP/#1-user-journey-orchestration-osme-orchestrator","title":"1. User Journey &amp; Orchestration (osme-orchestrator)","text":"<p>This is a new, high-level package that guides users through the platform's capabilities, acting as the main entry point.</p>"},{"location":"PROJECT_ROADMAP/#mvp-deliverable_6","title":"MVP Deliverable","text":"<ul> <li>A Command-Line Interface (CLI) wizard that interactively guides the user through the primary use cases:<ul> <li>\"I want to download data.\" -&gt; Guides user through weather_data_retrieval and grid_data_retrieval configuration.</li> <li>\"I have data and want to build a Marginal Emissions model.\" -&gt; Guides user through data_cleaning_and_joining and then into the marginal_emissions_modelling notebook templates.</li> <li>\"I have a model and data, and I want to run an optimisation.\" -&gt; Guides user through the optimisation module setup.</li> </ul> </li> <li>The wizard generates the necessary JSON configuration files for batch execution in other modules.</li> </ul>"},{"location":"PROJECT_ROADMAP/#future-development_6","title":"Future Development","text":"<ul> <li>Web-based UI: A simple Streamlit or Dash application that provides the same guided experience as the CLI wizard, with forms for data selection, model parameters, and optimisation constraints.</li> <li>\"Recipe\" System: Pre-defined, end-to-end JSON configurations for common scenarios (e.g., \"India 2023 MEF Model,\" \"UK Demand Shifting Optimisation\").</li> <li>Project Scaffolding: Automatically creates a well-structured project directory with data/raw, data/processed, models/, config/ folders.</li> </ul>"},{"location":"PROJECT_ROADMAP/#2-shared-infrastructure-core-utilities-osme-core","title":"2. Shared Infrastructure &amp; Core Utilities (osme-core)","text":"<p>This package contains the shared code that all other modules depend on.</p>"},{"location":"PROJECT_ROADMAP/#mvp-deliverable_7","title":"MVP Deliverable","text":"<ul> <li>Unified Configuration Management: A single, validated JSON schema for all module configurations. Handles credentials (via environment variables or a secure vault), data directories, and execution parameters.</li> <li>Standardized Logging: A common logging format and lifecycle (info, debug, warning, error) across all modules, with structured logging for easy parsing.</li> <li>Common Data Types &amp; Models: Pydantic models or Python dataclasses for core concepts like TemporalRange, GeographicalRegion, DataSource.</li> <li>Base Classes for Retrieval: Abstract base classes that define the interface for any data retrieval module, ensuring consistency.</li> </ul>"},{"location":"PROJECT_ROADMAP/#future-development_7","title":"Future Development","text":"<ul> <li>Plugin Architecture: A formal system for adding new data providers, solvers, or gap-filling methods without modifying the core code.</li> <li>Health &amp; Performance Monitoring: Utilities for tracking memory usage, runtime, and data validation metrics across modules.</li> <li>Serialization Utilities: Standardized methods for saving/loading models, optimisation results, and large datasets (e.g., using joblib or Apache Parquet).</li> </ul>"},{"location":"PROJECT_ROADMAP/#3-execution-compute-layer-osme-compute","title":"3. Execution &amp; Compute Layer (osme-compute)","text":"<p>This component abstracts the computational environment, enabling both local development and scalable cloud/HPC execution.</p>"},{"location":"PROJECT_ROADMAP/#mvp-deliverable_8","title":"MVP Deliverable","text":"<ul> <li>Local Execution Engine: The default mode, running all processes on the user's local machine. Includes the resource estimation warnings mentioned in the optimisation roadmap.</li> </ul>"},{"location":"PROJECT_ROADMAP/#future-development_8","title":"Future Development","text":"<ul> <li> <p>Workflow Orchestration: Integration with workflow engines like Prefect or Dagster to define the entire data pipeline (retrieve -&gt; clean -&gt; model -&gt; optimise) as a single, monitored, and re-runnable DAG (Directed Acyclic Graph).</p> </li> <li> <p>Cloud &amp; HPC Abstraction:</p> <ul> <li>Azure/AWS Batch Integration: Package and submit jobs to cloud batch services for at-scale runs.</li> <li>Slurm/HPC Support: Script generation and job submission for High-Performance Computing clusters.</li> <li>Data &amp; Result Caching: A smart caching layer to avoid re-downloading or re-processing data that has not changed.</li> </ul> </li> </ul>"},{"location":"osme-common-utilities-documentation/","title":"Overview","text":"<p>Currently under development!</p>"},{"location":"osme-common-utilities-documentation/codebase/","title":"Codebase Reference","text":"<p>This document provides an overview of the main components of the <code>data_cleaning_and_joining</code> package, detailing the primary modules and their functionalities.</p> <p>Currently under development!</p>"},{"location":"osme-common-utilities-documentation/quickstart/","title":"Quickstart","text":"<p>Currently under development!</p>"},{"location":"osme-data-cleaning-and-joining-documentation/","title":"Overview","text":"<p>Currently under development!</p>"},{"location":"osme-data-cleaning-and-joining-documentation/codebase/","title":"Codebase Reference","text":"<p>This document provides an overview of the main components of the <code>data_cleaning_and_joining</code> package, detailing the primary modules and their functionalities.</p> <p>Currently under development!</p>"},{"location":"osme-data-cleaning-and-joining-documentation/quickstart/","title":"Quickstart","text":"<p>Currently under development!</p>"},{"location":"osme-grid-data-retrieval-documentation/","title":"Overview","text":"<p>Currently under development!</p>"},{"location":"osme-grid-data-retrieval-documentation/codebase/","title":"Codebase Reference","text":"<p>Currently under development!</p>"},{"location":"osme-grid-data-retrieval-documentation/quickstart/","title":"Quickstart","text":"<p>Currently under development!</p>"},{"location":"osme-marginal-emissions-modelling-documentation/","title":"Overview","text":"<p>Currently under development!</p>"},{"location":"osme-marginal-emissions-modelling-documentation/codebase/","title":"Codebase Reference","text":"<p>This document provides an overview of the main components of the <code>marginal_emissions_modelling</code> package, detailing the primary modules and their functionalities.</p> <p>Currently under development!</p>"},{"location":"osme-marginal-emissions-modelling-documentation/quickstart/","title":"Quickstart","text":"<p>Currently under development!</p>"},{"location":"osme-weather-data-retrieval-documentation/","title":"Overview","text":"<p>Currently under development!</p>"},{"location":"osme-weather-data-retrieval-documentation/codebase/","title":"Codebase Reference","text":"<p>This document provides an overview of the main components of the <code>weather_data_retrieval</code> package, detailing the primary modules and their functionalities.</p>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.runner","title":"weather_data_retrieval.runner","text":""},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.runner.run","title":"run","text":"<pre><code>run(\n    config,\n    run_mode=\"interactive\",\n    verbose=True,\n    logger=None,\n)\n</code></pre> <p>Unified orchestration entry point for both interactive and automatic runs. Handles validation, logging, estimation, and download orchestration.</p> <p>Returns: 0=success, 1=fatal error, 2=some downloads failed.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary with all required parameters.</p> required <code>run_mode</code> <code>str</code> <p>Run mode, either 'interactive' or 'automatic', by default \"interactive\".</p> <code>'interactive'</code> <code>logger</code> <code>Logger</code> <p>Pre-configured logger instance, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Exit code: 0=success, 1=fatal error, 2=some downloads failed.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/runner.py</code> <pre><code>def run(\n        config: dict,\n        run_mode: str = \"interactive\",\n        verbose: bool = True,\n        logger=None\n        ) -&gt; int:\n    \"\"\"\n    Unified orchestration entry point for both interactive and automatic runs.\n    Handles validation, logging, estimation, and download orchestration.\n\n    Returns: 0=success, 1=fatal error, 2=some downloads failed.\n\n    Parameters\n    ----------\n    config : dict\n        Configuration dictionary with all required parameters.\n    run_mode : str, optional\n        Run mode, either 'interactive' or 'automatic', by default \"interactive\".\n    logger : logging.Logger, optional\n        Pre-configured logger instance, by default None.\n\n    Returns\n    -------\n    int\n        Exit code: 0=success, 1=fatal error, 2=some downloads failed.\n    \"\"\"\n\n    console_handler_active = (run_mode == \"interactive\") or verbose\n\n    def echo_only_if_no_console_handler(force: bool = False) -&gt; bool:\n        # In automatic non-verbose mode, console handler is absent.\n        # Return True to echo only selected messages (summary, warnings/errors/exceptions).\n        return force and (not console_handler_active)\n\n    save_base = config.get(\"save_dir\")\n    if save_base:\n        save_path = resolve_under(data_dir(create=True), save_base)\n    else:\n        save_path = data_dir(create=True)\n\n    if logger is None:\n        logger = setup_logger(str(log_dir(create=True)), run_mode=run_mode, verbose=verbose)\n\n    # Header\n    log_msg(\"=\" * 60, logger, echo_console=echo_only_if_no_console_handler(False))\n    log_msg(f\"Starting {run_mode.upper()} run at {datetime.now().isoformat()}\",\n            logger, echo_console=echo_only_if_no_console_handler(False))\n    log_msg(\"=\" * 60, logger, echo_console=echo_only_if_no_console_handler(False))\n\n    try:\n        # 1) Validate config\n        validate_config(config, logger=logger, run_mode=run_mode)\n        log_msg(\"Configuration validation successful.\", logger, echo_console=echo_only_if_no_console_handler(False))\n\n        # 2) Map config \u2192 session\n        session = SessionState()\n        ok, notes = map_config_to_session(config, session, logger=logger)\n        for note in notes:\n            log_msg(note, logger, echo_console=echo_only_if_no_console_handler(False))\n        if not ok:\n            log_msg(\"Config mapping reported blocking issues. Exiting.\",\n                    logger, level=\"error\", echo_console=echo_only_if_no_console_handler(True))\n            if \"filename_base\" in locals():\n                create_final_log_file(session, filename_base, logger, delete_original=True, reattach_to_final=True)\n            else:\n                log_msg(\"Skipping final log file creation because filename_base was not defined (run failed early).\", logger)\n            return 1\n\n        # 2b) Automatic mode cannot be case-by-case for existing file policy (already coerced by validate_config)\n        # Nothing to do here; messages already logged.\n\n        # 3) Short internet speed test (both modes)\n        speed_mbps = internet_speedtest(test_urls=None, max_seconds=10, logger=logger, echo_console=echo_only_if_no_console_handler(False))\n        log_msg(f\"Detected speed: {speed_mbps:.1f} Mbps\", logger, echo_console=echo_only_if_no_console_handler(False))\n\n        dataset_short_name = session.get(\"dataset_short_name\")\n        if dataset_short_name == \"era5-world\":\n            grid_res = 0.25\n        elif dataset_short_name == \"era5-land\":\n            grid_res = 0.1\n        else:\n            raise ValueError(f\"Unknown dataset_short_name: {dataset_short_name}\")\n        # 4) Estimate size/time\n        estimates = estimate_cds_download(\n            variables=session.get(\"variables\"),\n            area=session.get(\"region_bounds\"),\n            start_date=session.get(\"start_date\"),\n            end_date=session.get(\"end_date\"),\n            observed_speed_mbps=speed_mbps,\n            grid_resolution=grid_res,\n        )\n\n        # Adjust for parallelisation \u2014 scale total_time_sec only\n        parallel_conf = session.get(\"parallel_settings\")\n        if parallel_conf and parallel_conf.get(\"enabled\"):\n            efficiency_factor = 0.60\n            max_conc = max(1, int(parallel_conf[\"max_concurrent\"]))\n            estimates[\"total_time_sec\"] = estimates[\"total_time_sec\"] / (max_conc * efficiency_factor)\n            log_msg(\n                f\"Adjusted total time for parallel downloads: {format_duration(estimates['total_time_sec'])}\",\n                logger, echo_console=echo_only_if_no_console_handler(False)\n            )\n\n        # 5) Filename + hash\n        coord_str = format_coordinates_nwse(boundaries=session.get(\"region_bounds\"))\n        hash_str = generate_filename_hash(\n            dataset_short_name=session.get(\"dataset_short_name\"),\n            variables=session.get(\"variables\"),\n            boundaries=session.get(\"region_bounds\"),\n        )\n        filename_base = f\"{session.get('dataset_short_name')}_{coord_str}_{hash_str}\"\n\n        # 6) Summary (ALWAYS printed in interactive/verbose; printed in non-verbose via echo)\n        summary = build_download_summary(session, estimates, speed_mbps)\n        log_msg(msg=summary, logger=logger, echo_console=echo_only_if_no_console_handler(force=True))\n        log_msg(msg=f\"Output base filename: {filename_base}\", logger=logger, echo_console=echo_only_if_no_console_handler(True))\n\n        # 7) Downloads\n        successful, failed, skipped = [], [], []\n\n        log_msg(\"\\n\" + \"-\" * 60 + \"\\n\\n\\nBeginning download process...\", logger, echo_console=echo_only_if_no_console_handler(False))\n        orchestrate_cds_downloads(\n            session=session,\n            filename_base=filename_base,\n            successful_downloads=successful,\n            failed_downloads=failed,\n            skipped_downloads=skipped,\n            logger=logger,\n            echo_console=echo_only_if_no_console_handler(False),  # internal steps won\u2019t echo in non-verbose\n            allow_prompts=(run_mode == \"interactive\"),\n        )\n\n        # Final counts \u2014 treat as summary (echo in non-verbose)\n        log_msg(\"-\" * 60, logger, echo_console=echo_only_if_no_console_handler(True))\n        log_msg(\"Download process completed.\", logger, echo_console=echo_only_if_no_console_handler(True))\n        log_msg(f\"Successful : {len(successful)}\", logger, echo_console=echo_only_if_no_console_handler(True))\n        log_msg(f\"Skipped    : {len(skipped)}\", logger, echo_console=echo_only_if_no_console_handler(True))\n        log_msg(f\"Failed     : {len(failed)}\", logger, echo_console=echo_only_if_no_console_handler(True))\n        log_msg(\"-\" * 60, logger, echo_console=echo_only_if_no_console_handler(True))\n\n        if failed:\n            log_msg(\"Some downloads failed. Review logs for details.\",\n                    logger, level=\"warning\", echo_console=echo_only_if_no_console_handler(True))\n            create_final_log_file(session, filename_base, logger, delete_original=True, reattach_to_final=True)\n            log_msg(msg=\"\\nProgram ended, goodbye.\\n\\n\", logger=logger, echo_console=echo_only_if_no_console_handler(False))\n            return 2\n        create_final_log_file(session, filename_base, logger, delete_original=True, reattach_to_final=True)\n        log_msg(msg=\"*\"*60 + \"\\nProgram completed, thank you for using this tool. Goodbye!\\n\" + \"*\"*60 + \"\\n\\n\", logger=logger, echo_console=echo_only_if_no_console_handler(False))\n\n\n    except Exception as e:\n        # Always echo errors in non-verbose mode\n        log_msg(f\"Run failed with exception: {e}\", logger, level=\"exception\", echo_console=echo_only_if_no_console_handler(True))\n        coord_str = format_coordinates_nwse(session.get(\"region_bounds\"))\n        hash_str = generate_filename_hash(\n            dataset_short_name=session.get(\"dataset_short_name\"),\n            variables=session.get(\"variables\"),\n            boundaries=session.get(\"region_bounds\"),\n        )\n        filename_base = f\"{session.get('dataset_short_name')}_{coord_str}_{hash_str}\"\n        create_final_log_file(session, filename_base, logger, delete_original=True, reattach_to_final=True)\n        log_msg(\"\\nProgram ended, goodbye.\\n\\n\", logger, echo_console=echo_only_if_no_console_handler(True))\n        return 1\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.runner.run_batch_from_config","title":"run_batch_from_config","text":"<pre><code>run_batch_from_config(cfg_path, logger=None)\n</code></pre> <p>Run automatic batch from a config file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary with all required parameters.</p> required <code>logger</code> <code>Logger</code> <p>Pre-configured logger instance, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Exit code: 0=success, 1=fatal error, 2=some downloads failed.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/runner.py</code> <pre><code>def run_batch_from_config(\n        cfg_path: str,\n        logger=None\n        ) -&gt; int:\n    \"\"\"\n    Run automatic batch from a config file.\n\n    Parameters\n    ----------\n    config : dict\n        Configuration dictionary with all required parameters.\n    logger : logging.Logger, optional\n        Pre-configured logger instance, by default None.\n\n    Returns\n    -------\n    int\n        Exit code: 0=success, 1=fatal error, 2=some downloads failed.\n    \"\"\"\n    config = load_config(cfg_path)\n    return run(config, run_mode=\"automatic\", verbose=False, logger=logger)\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.cli","title":"weather_data_retrieval.io.cli","text":""},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.cli.parse_args","title":"parse_args","text":"<pre><code>parse_args()\n</code></pre> <p>Parse command-line arguments.</p> <p>Parameters:</p> Name Type Description Default <code>None</code> required <p>Returns:</p> Type Description <code>Namespace</code> <p>Parsed arguments.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/cli.py</code> <pre><code>def parse_args() -&gt; argparse.Namespace:\n    \"\"\"\n    Parse command-line arguments.\n\n    Parameters\n    ----------\n    None\n\n    Returns\n    -------\n    argparse.Namespace\n        Parsed arguments.\n    \"\"\"\n    p = argparse.ArgumentParser(\n        description=\"ERA5/Open-Meteo downloader \u2014 interactive or batch via config file.\"\n    )\n    p.add_argument(\n        \"--config\",\n        nargs=\"?\",\n        default=None,\n        help=\"Path to JSON config file. If provided, runs in non-interactive (automatic) mode.\"\n    )\n    p.add_argument(\n        \"--log-dir\",\n        default=None,\n        help=\"Directory where logs will be written (file name is auto-generated).\"\n    )\n    p.add_argument(\n        \"--verbose\",\n        action=\"store_true\",\n        help=\"Automatic mode only: also echo log messages to console and show a prompt-style transcript (no input).\"\n    )\n    return p.parse_args()\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.cli.run_prompt_wizard","title":"run_prompt_wizard","text":"<pre><code>run_prompt_wizard(session, logger=None)\n</code></pre> <p>Drives the interactive prompt flow (no config-source step). Returns True if all fields completed; False if user exits.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>The session state to populate.</p> required <code>logger</code> <code>Logger</code> <p>Logger for logging messages, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if completed; False if exited early.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/cli.py</code> <pre><code>def run_prompt_wizard(\n        session: SessionState,\n        logger: logging.Logger = None,\n    ) -&gt; bool:\n    \"\"\"\n    Drives the interactive prompt flow (no config-source step).\n    Returns True if all fields completed; False if user exits.\n\n    Parameters\n    ----------\n    session : SessionState\n        The session state to populate.\n    logger : logging.Logger, optional\n        Logger for logging messages, by default None.\n\n    Returns\n    -------\n    bool\n        True if completed; False if exited early.\n    \"\"\"\n\n    log_msg(\"\\n\\n\" + \"=\" * 60, logger)\n    log_msg(\"Welcome to the Weather Data Retrieval Prompt Wizard!\\n\" + \"=\"*60, logger)\n    log_msg(\"\\nPlease follow the prompts to configure your data retrieval settings.\\n\", logger)\n    log_msg(\"At any point, you may type:\\n   'back' to return to the previous prompt.\", logger)\n    log_msg(\"   'exit' to quit the wizard\\n   'Ctrl+C' to stop the program.\\n\", logger)\n    log_msg(\"=\" * 60 + \"\\n\", logger)\n\n\n    while True:\n        key = session.first_unfilled_key()\n        if key is None:\n            return True\n\n        if key == \"data_provider\":\n            res = prompt_data_provider(session, logger=logger)\n            if res == \"__EXIT__\": return False\n            if res == \"__BACK__\": continue\n\n        elif key == \"dataset_short_name\":\n            provider = session.get(\"data_provider\")\n            res = prompt_dataset_short_name(session, provider, logger=logger)\n            if res == \"__EXIT__\": return False\n            if res == \"__BACK__\":\n                session.unset(\"data_provider\")\n                continue\n\n        # CDS-specific prompts\n        elif session.get(\"data_provider\") == \"cds\":\n            if key == \"api_url\":\n                res_url = prompt_cds_url(session, \"https://cds.climate.copernicus.eu/api\", logger=logger)\n                if res_url == \"__EXIT__\": return False\n                if res_url == \"__BACK__\":\n                    session.unset(\"dataset_short_name\")\n                    continue\n\n            elif key == \"api_key\":\n                res_key = prompt_cds_api_key(session, logger=logger)\n                if res_key == \"__EXIT__\": return False\n                if res_key == \"__BACK__\":\n                    session.unset(\"api_url\")\n                    continue\n\n                client = validate_cds_api_key(session.get(\"api_url\"), session.get(\"api_key\"), logger=logger)\n                if client is None:\n                    log_msg(\"Authentication failed. Please re-enter your API details.\\n\", logger)\n                    session.unset(\"api_key\")\n                    session.unset(\"api_url\")\n                    continue\n                session.set(\"session_client\", client)\n\n        # Open-Meteo-specific prompts\n        elif session.get(\"data_provider\") == \"open-meteo\":\n            raise NotImplementedError(\"Open-Meteo variable validation not yet implemented.\")\n\n        if key == \"save_dir\":\n            raw_save_path = prompt_save_directory(session, default_save_dir, logger=logger)\n            if raw_save_path in (\"__EXIT__\", \"__BACK__\"):\n                if raw_save_path == \"__BACK__\":\n                    session.unset(\"session_client\")\n                    session.unset(\"api_key\")\n                    continue\n                return False\n\n            # Normalize and resolve path safely\n            resolved_path = resolve_under(data_dir(create=True), raw_save_path)\n            session.set(\"save_dir\", str(resolved_path))\n            log_msg(f\"Save directory resolved to: {resolved_path}\", logger)\n            continue\n\n\n        elif key == \"start_date\":\n            # This prompt sets BOTH start_date and end_date\n            s, e = prompt_date_range(session, logger=logger)\n            if s == \"__EXIT__\": return False\n            if s == \"__BACK__\":\n                session.unset(\"save_dir\")\n                continue\n\n        elif key == \"end_date\":\n            # Will already be filled by prompt_date_range; just skip\n            continue\n\n        elif key == \"region_bounds\":\n            bounds = prompt_coordinates(session, logger=logger)\n            if bounds == \"__EXIT__\": return False\n            if bounds == \"__BACK__\":\n                session.unset(\"start_date\")\n                session.unset(\"end_date\")\n                continue\n\n        elif key == \"variables\":\n            if session.get(\"dataset_short_name\") == \"era5-world\":\n                invalid_vars = invalid_era5_world_variables\n            elif session.get(\"dataset_short_name\") == \"era5-land\":\n                invalid_vars = invalid_era5_land_variables\n            else:\n                raise ValueError(\"Unknown dataset for variable validation.\")\n            variables = prompt_variables(session, invalid_vars, logger=logger)\n            if variables in (\"__EXIT__\", \"__BACK__\"):\n                if variables == \"__BACK__\":\n                    session.unset(\"region_bounds\")\n                    continue\n                return False\n\n        elif key == \"existing_file_action\":\n            efa = prompt_skip_overwrite_files(session, logger=logger)\n            if efa in (\"__EXIT__\", \"__BACK__\"):\n                if efa == \"__BACK__\":\n                    session.unset(\"variables\")\n                    continue\n                return False\n\n        elif key == \"parallel_settings\":\n            ps = prompt_parallelisation_settings(session, logger=logger)\n            if ps in (\"__EXIT__\", \"__BACK__\"):\n                if ps == \"__BACK__\":\n                    session.unset(\"existing_file_action\")\n                    continue\n                return False\n\n        elif key == \"retry_settings\":\n            rs = prompt_retry_settings(session, logger=logger)\n            if rs in (\"__EXIT__\", \"__BACK__\"):\n                if rs == \"__BACK__\":\n                    session.unset(\"parallel_settings\")\n                    continue\n                return False\n\n        elif key == \"inputs_confirmed\":\n            log_msg(\"\\n\" + \"*\" * 60 + \"\\n\" + \"*\" * 60 + \"\\n\", logger)\n            log_msg(\"\\nPrompting wizard complete. Please review your selections:\\n\", logger)\n            rs = prompt_continue_confirmation(session=session, logger=logger)\n            if rs in (\"__EXIT__\", \"__BACK__\"):\n                if rs == \"__BACK__\":\n                    session.unset(\"parallel_settings\")\n                    continue\n                return False\n            session.set(\"inputs_confirmed\", True)\n            log_msg(\"\\nSelections confirmed.\", logger)\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.config_loader","title":"weather_data_retrieval.io.config_loader","text":""},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.config_loader.load_and_validate_config","title":"load_and_validate_config","text":"<pre><code>load_and_validate_config(\n    path, *, logger=None, run_mode=\"automatic\"\n)\n</code></pre> <p>Load JSON config and validate it using the centralized validator. This lets the validator log coercions/warnings (e.g., case_by_case \u2192 skip_all).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to JSON config file.</p> required <code>logger</code> <code>Logger</code> <p>Logger instance for validation messages, by default None.</p> <code>None</code> <code>run_mode</code> <code>str</code> <p>Run mode, either 'interactive' or 'automatic', by default \"automatic\".</p> <code>'automatic'</code> <p>Returns:</p> Type Description <code>dict</code> <p>Validated configuration dictionary.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/config_loader.py</code> <pre><code>def load_and_validate_config(\n        path: str,\n        *,\n        logger=None,\n        run_mode: str = \"automatic\") -&gt; dict:\n    \"\"\"\n    Load JSON config and validate it using the centralized validator.\n    This lets the validator log coercions/warnings (e.g., case_by_case \u2192 skip_all).\n\n    Parameters\n    ----------\n    path : str\n        Path to JSON config file.\n    logger : logging.Logger, optional\n        Logger instance for validation messages, by default None.\n    run_mode : str, optional\n        Run mode, either 'interactive' or 'automatic', by default \"automatic\".\n\n    Returns\n    -------\n    dict\n        Validated configuration dictionary.\n    \"\"\"\n    cfg_path = _resolve_config_path(path)\n    with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n        config = json.load(f)\n    # Let validate_config perform normalization/clamping with logging context\n    validate_config(config, logger=logger, run_mode=run_mode)\n    return config\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.config_loader.load_config","title":"load_config","text":"<pre><code>load_config(file_path)\n</code></pre> <p>Load configuration from a JSON requirements file (without validation).</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to JSON config file.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Configuration dictionary.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/config_loader.py</code> <pre><code>def load_config(file_path: str) -&gt; dict:\n    \"\"\"\n    Load configuration from a JSON requirements file (without validation).\n\n    Parameters\n    ----------\n    file_path : str\n        Path to JSON config file.\n\n    Returns\n    -------\n    dict\n        Configuration dictionary.\n    \"\"\"\n    # if not os.path.exists(file_path):\n    #     raise FileNotFoundError(f\"Configuration file not found: {file_path}\")\n    # with open(file_path, \"r\", encoding=\"utf-8\") as f:\n    #     return json.load(f)\n    cfg_path = _resolve_config_path(file_path)\n    with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n        return json.load(f)\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.prompts","title":"weather_data_retrieval.io.prompts","text":""},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.prompts.read_input","title":"read_input","text":"<pre><code>read_input(prompt, *, logger=None)\n</code></pre> <p>Centralized input handler with built-in 'exit' and 'back' controls.</p> Parameters: <p>prompt : str     The prompt to display to the user. logger : logging.Logger, optional     Logger to log the prompt message.</p> Returns: <p>str     The user input, or special command indicators.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/prompts.py</code> <pre><code>def read_input(\n        prompt: str,\n        *,\n        logger=None,\n        ) -&gt; str:\n    \"\"\"\n    Centralized input handler with built-in 'exit' and 'back' controls.\n\n    Parameters:\n    ----------\n    prompt : str\n        The prompt to display to the user.\n    logger : logging.Logger, optional\n        Logger to log the prompt message.\n\n    Returns:\n    -------\n    str\n        The user input, or special command indicators.\n    \"\"\"\n    # Log to file only (DEBUG won't be shown by the console handler at INFO)\n    if logger:\n        # Use level=\"debug\" so it doesn't appear on console\n        log_msg(prompt, logger, level=\"debug\", echo_console=False)\n\n    # Show the prompt exactly once on the console\n    raw = input(prompt).strip()\n    lower = raw.lower()\n\n    if lower in (\"exit\", \"quit\"):\n        return \"__EXIT__\"\n    if lower == \"back\":\n        return \"__BACK__\"\n\n    return lower\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.prompts.say","title":"say","text":"<pre><code>say(text, *, logger=None)\n</code></pre> <p>Centralized output handler to log and print messages.</p> Parameters: <p>text : str     The message to display. logger : logging.Logger, optional     Logger to log the message.</p> Returns: <p>None</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/prompts.py</code> <pre><code>def say(\n        text: str,\n        *,\n        logger=None\n        ) -&gt; None:\n    \"\"\"\n    Centralized output handler to log and print messages.\n\n    Parameters:\n    ----------\n    text : str\n        The message to display.\n    logger : logging.Logger, optional\n        Logger to log the message.\n\n    Returns:\n    -------\n    None\n\n    \"\"\"\n    # if logger:\n    #     logger.info(text)\n    log_msg(text, logger)\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.prompts.prompt_data_provider","title":"prompt_data_provider","text":"<pre><code>prompt_data_provider(session, *, logger=None)\n</code></pre> <p>Prompt user for which data provider to use (CDS or Open-Meteo).</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>Current session state to store selected data provider.</p> required <code>logger</code> <code>Logger</code> <p>Logger for logging messages, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Normalized provider name (\"cds\" or \"open-meteo\"), or special control token \"BACK\" or \"EXIT\".</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/prompts.py</code> <pre><code>def prompt_data_provider(\n        session: SessionState,\n        *,\n        logger=None,\n        ) -&gt; str:\n    \"\"\"\n    Prompt user for which data provider to use (CDS or Open-Meteo).\n\n    Parameters\n    ----------\n    session : SessionState\n        Current session state to store selected data provider.\n    logger : logging.Logger, optional\n        Logger for logging messages, by default None.\n\n    Returns\n    -------\n    str\n        Normalized provider name (\"cds\" or \"open-meteo\"),\n        or special control token \"__BACK__\" or \"__EXIT__\".\n\n    \"\"\"\n\n    say(\"-\"*60 + \"\\nData Provider Selection:\\n\" + '-'*60, logger=logger)\n    say(\"Available data providers:\\n\\t1. Copernicus Climate Data Store (CDS)\\n\\t2. Open-Meteo\", logger=logger)\n\n    while True:\n        raw = read_input(\"\\nPlease enter the data provider you would like to use (name or number): \",\n                         logger=logger)\n\n        if raw in (\"__EXIT__\", \"__BACK__\"):\n            return raw\n\n        data_provider = normalize_input(raw, \"data_provider\")\n\n        if not validate_data_provider(data_provider):\n            say(\"\\nERROR: Invalid provider. Please enter '1' for CDS or '2' for Open-Meteo\", logger=logger)\n            continue\n        if data_provider == \"open-meteo\":\n            say(\"\\nERROR: Open-Meteo support is not yet implemented. Please select CDS.\", logger=logger)\n            continue\n\n        say(f\"\\nYou selected: [{data_provider.upper()}]\\n\", logger=logger)\n\n        session.set(\"data_provider\", data_provider)\n\n        return data_provider\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.prompts.prompt_dataset_short_name","title":"prompt_dataset_short_name","text":"<pre><code>prompt_dataset_short_name(\n    session, provider, *, logger=None\n)\n</code></pre> <p>Prompt for dataset choice.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>Current session state to store selected dataset.</p> required <code>provider</code> <code>str</code> <p>Data provider name.</p> required <code>logger</code> <code>Logger</code> <p>Logger for logging messages, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>    str: Normalized dataset name or 'exit' / 'back'.</code> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/prompts.py</code> <pre><code>def prompt_dataset_short_name(\n        session: SessionState,\n        provider: str,\n        *,\n        logger=None,\n        ) -&gt; str:\n    \"\"\"\n    Prompt for dataset choice.\n\n    Parameters\n    ----------\n    session: SessionState\n        Current session state to store selected dataset.\n    provider : str\n        Data provider name.\n    logger : logging.Logger, optional\n        Logger for logging messages, by default None.\n\n    Returns\n    -------\n        str: Normalized dataset name or 'exit' / 'back'.\n\n    \"\"\"\n    if provider != \"cds\":\n        say(\"\\nCurrently only CDS datasets are supported.\", logger=logger)\n        return \"__BACK__\"\n\n    say(\"-\"*60 + \"\\nDataset Selection:\\n\" + '-'*60, logger=logger)\n    say(\"Available CDS datasets:\\n\\t1. ERA5-Land\\n\\t2. ERA5-World\", logger=logger)\n\n    while True:\n        raw = read_input(\"\\nPlease enter the dataset you would like to use (name or number): \",\n                         logger=logger)\n        if raw in (\"__EXIT__\", \"__BACK__\"):\n            return raw\n\n        dataset_short_name = normalize_input(raw, \"era5_dataset_short_name\")\n        if not validate_dataset_short_name(dataset_short_name, provider):\n            say(\"\\nERROR: Invalid or unsupported dataset. Try again.\", logger=logger)\n            continue\n\n        # if dataset_short_name != \"era5-world\":\n        #     say(\"\\nERROR: Only ERA5-World dataset is implemented in this version. Please select ERA5-World.\", logger=logger)\n        #     continue\n\n        say(f\"\\nYou selected: [{dataset_short_name.upper()}]\\n\", logger=logger)\n        session.set(\"dataset_short_name\", dataset_short_name)\n        return dataset_short_name\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.prompts.prompt_cds_url","title":"prompt_cds_url","text":"<pre><code>prompt_cds_url(\n    session,\n    api_url_default=\"https://cds.climate.copernicus.eu/api\",\n    *,\n    logger=None\n)\n</code></pre> <p>Prompt for CDS API URL.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>Current session state to store API URL.</p> required <code>api_url_default</code> <code>str</code> <p>Default CDS API URL. https://cds.climate.copernicus.eu/api</p> <code>'https://cds.climate.copernicus.eu/api'</code> <p>Returns:</p> Type Description <code>    str: CDS API URL or 'exit' / 'back'.</code> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/prompts.py</code> <pre><code>def prompt_cds_url(\n        session: SessionState,\n        api_url_default: str = \"https://cds.climate.copernicus.eu/api\",\n        *,\n        logger=None,\n        ) -&gt; str:\n    \"\"\"\n    Prompt for CDS API URL.\n\n    Parameters\n    ----------\n    session : SessionState\n        Current session state to store API URL.\n    api_url_default : str\n        Default CDS API URL. https://cds.climate.copernicus.eu/api\n\n    Returns\n    -------\n        str: CDS API URL or 'exit' / 'back'.\n\n    \"\"\"\n    say(\"-\"*60 + \"\\nCDS API url:\\n\" + '-'*60, logger=logger)\n    say(f\"Default: {api_url_default}\", logger=logger)\n    while True:\n        raw = read_input(\"\\nEnter the CDS API url (or press Enter to keep default): \",\n                         logger=logger)\n        if raw in (\"__EXIT__\", \"__BACK__\"):\n            return raw\n        url = raw or api_url_default\n        session.set(\"api_url\", url)\n        say(f\"\\nYou entered the url : {url}\\n\", logger=logger)\n        return url\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.prompts.prompt_cds_api_key","title":"prompt_cds_api_key","text":"<pre><code>prompt_cds_api_key(session, *, logger=None)\n</code></pre> <p>Prompt only for the CDS API key (hidden input).</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>Current session state to store API key.</p> required <code>logger</code> <code>Logger</code> <p>Logger for logging messages, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>CDS API key or 'exit' / 'back'.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/prompts.py</code> <pre><code>def prompt_cds_api_key(\n        session: SessionState,\n        *,\n        logger=None,\n        ) -&gt; str:\n    \"\"\"\n    Prompt only for the CDS API key (hidden input).\n\n    Parameters\n    ----------\n    session : SessionState\n        Current session state to store API key.\n    logger : logging.Logger, optional\n        Logger for logging messages, by default None.\n\n    Returns\n    -------\n    str\n        CDS API key or 'exit' / 'back'.\n    \"\"\"\n    say(\"-\"*60 + \"\\nCDS API key:\\n\" + '-'*60, logger=logger)\n    while True:\n        key = getpass(\"\\nEnter your CDS API key: \")\n        # getpass cannot log input value; we just log the action\n        low = key.lower()\n        if low in (\"exit\", \"quit\"):\n            return \"__EXIT__\"\n        if low == \"back\":\n            return \"__BACK__\"\n        if not key:\n            say(\"\\nERROR: No API key entered. Please try again.\", logger=logger)\n            continue\n        session.set(\"api_key\", key)\n        say(f\"\\nYou entered an API key of length {len(key)} characters.\\n\", logger=logger)\n        return key\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.prompts.prompt_save_directory","title":"prompt_save_directory","text":"<pre><code>prompt_save_directory(session, default_dir, *, logger=None)\n</code></pre> <p>Ask for save directory, create if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>Current session state to store save directory.</p> required <code>default_dir</code> <code>Path</code> <p>Default directory to suggest.</p> required <code>logger</code> <code>Logger</code> <p>Logger for logging messages, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path | str</code> <p>Path to save directory, or control token \"BACK\" / \"EXIT\".</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/prompts.py</code> <pre><code>def prompt_save_directory(\n        session: SessionState,\n        default_dir: Path,\n        *,\n        logger=None,\n    ) -&gt; Path | str:\n\n    \"\"\"\n    Ask for save directory, create if necessary.\n\n    Parameters\n    ----------\n    session : SessionState\n        Current session state to store save directory.\n    default_dir : Path\n        Default directory to suggest.\n    logger : logging.Logger, optional\n        Logger for logging messages, by default None.\n\n    Returns\n    -------\n    Path | str\n        Path to save directory, or control token \"__BACK__\" / \"__EXIT__\".\n\n    \"\"\"\n    say(\"-\"*60 + \"\\nData Save Directory Selection:\\n\" + '-'*60, logger=logger)\n    say(f\"Default: {default_dir}\", logger=logger)\n    while True:\n        raw = read_input(\"\\nEnter a path (or press Enter to use default): \", logger=logger)\n        if raw in (\"__EXIT__\", \"__BACK__\"):\n            return raw\n        # resolve under repo_root/data if relative\n        resolved_path = resolve_under(data_dir(create=True), raw or default_dir)\n        if validate_directory(str(resolved_path)):\n            say(f\"\\nYou set the save directory to: {resolved_path}\\n\", logger=logger)\n            session.set(\"save_dir\", resolved_path)\n            return resolved_path\n        say(f\"ERROR: Directory [{resolved_path}] could not be created or accessed. Try another path.\", logger=logger)\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.prompts.prompt_date_range","title":"prompt_date_range","text":"<pre><code>prompt_date_range(session, *, logger=None)\n</code></pre> <p>Ask user for start and end date, with validation. Accepts formats: YYYY-MM-DD or YYYY-MM - Start dates without day default to first day of month (YYYY-MM-01) - End dates without day default to last day of month (YYYY-MM-[last day])</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>Current session state to store date range.</p> required <code>logger</code> <code>Logger</code> <p>Logger for logging messages, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>(start_date_str, end_date_str) in ISO format (YYYY-MM-DD), or (\"EXIT\", \"EXIT\") / (\"BACK\", \"BACK\")</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/prompts.py</code> <pre><code>def prompt_date_range(\n        session: SessionState,\n        *,\n        logger=None,\n        ) -&gt; tuple[str, str]:\n    \"\"\"\n    Ask user for start and end date, with validation.\n    Accepts formats: YYYY-MM-DD or YYYY-MM\n    - Start dates without day default to first day of month (YYYY-MM-01)\n    - End dates without day default to last day of month (YYYY-MM-[last day])\n\n    Parameters\n    ----------\n    session : SessionState\n        Current session state to store date range.\n    logger : logging.Logger, optional\n        Logger for logging messages, by default None.\n\n    Returns\n    -------\n    tuple[str, str]\n        (start_date_str, end_date_str) in ISO format (YYYY-MM-DD),\n        or (\"__EXIT__\", \"__EXIT__\") / (\"__BACK__\", \"__BACK__\")\n    \"\"\"\n    say(\"-\"*60 + \"\\nDate Range Selection:\\n\" + '-'*60, logger=logger)\n    say(\"Enter dates as YYYY-MM-DD or YYYY-MM\\n(YYYY-MM will default to first day for start, last day for end)\", logger=logger)\n\n    while True:\n        start_raw = read_input(\"\\nEnter start date: \", logger=logger)\n        if start_raw in (\"__EXIT__\", \"__BACK__\"): return start_raw, start_raw\n        end_raw = read_input(\"\\nEnter end date: \", logger=logger)\n        if end_raw in (\"__EXIT__\", \"__BACK__\"): return end_raw, end_raw\n\n        if not validate_date(start_raw, allow_month_only=True):\n            say(\"\\nERROR: Invalid start date format. Use YYYY-MM-DD or YYYY-MM.\", logger=logger)\n            continue\n        if not validate_date(end_raw, allow_month_only=True):\n            say(\"\\nERROR: Invalid end date format. Use YYYY-MM-DD or YYYY-MM.\", logger=logger)\n            continue\n\n        try:\n            start, start_str = parse_date_with_defaults(start_raw, default_to_month_end=False)\n            if len(start_raw) == 7:\n                say(f\"\\n\\tStart date set to: {start_str} (first day of month)\", logger=logger)\n            end, end_str = parse_date_with_defaults(end_raw, default_to_month_end=True)\n            if len(end_raw) == 7:\n                say(f\"\\tEnd date set to: {end_str} (last day of month)\\n\", logger=logger)\n        except ValueError as e:\n            say(f\"\\nERROR: Could not parse dates: {e}\", logger=logger)\n            continue\n\n        if end &lt;= start:\n            say(\"\\nERROR: End date must be after start date.\", logger=logger)\n            continue\n\n        if session.get(\"data_provider\") == \"cds\":\n            end = clamp_era5_available_end_date(end)\n            end_str = end.date().isoformat()\n\n        say(f\"\\nYou selected a date range of start [{start.date().isoformat()}] \u2192 end [{end.date().isoformat()}]\\n\",\n            logger=logger)\n        session.set(\"start_date\", start.date().isoformat())\n        session.set(\"end_date\", end.date().isoformat())\n        return start.date().isoformat(), end.date().isoformat()\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.prompts.prompt_coordinates","title":"prompt_coordinates","text":"<pre><code>prompt_coordinates(session, *, logger=None)\n</code></pre> <p>Prompt user for geographic boundaries (N, S, W, E) with validation.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>Current session state to store geographic boundaries.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>[north, west, south, east] boundaries or special tokens \"EXIT\" / \"BACK\".</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/prompts.py</code> <pre><code>def prompt_coordinates(\n        session: SessionState,\n        *,\n        logger=None,\n        ) -&gt; list[float]:\n    \"\"\"\n    Prompt user for geographic boundaries (N, S, W, E) with validation.\n\n    Parameters\n    ----------\n    session : SessionState\n        Current session state to store geographic boundaries.\n\n    Returns\n    -------\n    list[float]\n        [north, west, south, east] boundaries or special tokens \"__EXIT__\" / \"__BACK__\".\n    \"\"\"\n    say('-'*60 + \"\\nGrid Area Selection (EPSG: 4326):\\n\" + '-'*60, logger=logger)\n    while True:\n        entries = {}\n        for label, key in [(\"Northern latitude\", \"north\"),\n                           (\"Southern latitude\", \"south\"),\n                           (\"Western longitude\", \"west\"),\n                           (\"Eastern longitude\", \"east\")]:\n            value = read_input(f\"\\nEnter {label} boundary: \", logger=logger)\n            if value in (\"__BACK__\", \"__EXIT__\"):\n                return value\n            entries[key] = value\n\n        try:\n            n, s, w, e = (\n                float(entries[\"north\"]),\n                float(entries[\"south\"]),\n                float(entries[\"west\"]),\n                float(entries[\"east\"]),\n            )\n        except ValueError:\n            say(\"\\nPlease enter numeric values for all coordinates.\", logger=logger)\n            continue\n\n        if not validate_coordinates(n, w, s, e):\n            say(\"Invalid bounds. Check that -90 \u2264 lat \u2264 90, -180 \u2264 lon \u2264 180, and North &gt; South.\", logger=logger)\n            continue\n\n        bounds = [n, w, s, e]\n        say(f\"You entered boundaries of: [N{n}, W{w}, S{s}, E{e}]\\n\", logger=logger)\n        session.set(\"region_bounds\", bounds)\n        return bounds\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.prompts.prompt_variables","title":"prompt_variables","text":"<pre><code>prompt_variables(\n    session,\n    variable_restrictions_list,\n    *args,\n    restriction_allow=False,\n    logger=None\n)\n</code></pre> <p>Ask for variables to download, validate each against allowed/disallowed list, and only update session if the full set is valid.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>Current session state to store selected variables.</p> required <code>variable_restrictions_list</code> <code>list[str]</code> <p>List of variables that are either allowed or disallowed.</p> required <code>restriction_allow</code> <code>bool</code> <p>If True, variable_restrictions_list is an allowlist (i.e. in). If False, it's a denylist (i.e. not in)</p> <code>False</code> <code>logger</code> <code>Logger</code> <p>Logger for logging messages, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str] | str</code> <p>List of selected variable names, or control token \"BACK\" / \"EXIT\".</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/prompts.py</code> <pre><code>def prompt_variables(\n        session: SessionState,\n        variable_restrictions_list: list[str],\n        *args,\n        restriction_allow: bool = False,\n        logger=None,\n        ) -&gt; list[str] | str:\n    \"\"\"\n    Ask for variables to download, validate each against allowed/disallowed list,\n    and only update session if the full set is valid.\n\n    Parameters\n    ----------\n    session : SessionState\n        Current session state to store selected variables.\n    variable_restrictions_list : list[str]\n        List of variables that are either allowed or disallowed.\n    restriction_allow : bool\n        If True, variable_restrictions_list is an allowlist (i.e. in).\n        If False, it's a denylist (i.e. not in)\n    logger : logging.Logger, optional\n        Logger for logging messages, by default None.\n\n    Returns\n    -------\n    list[str] | str\n        List of selected variable names, or control token \"__BACK__\" / \"__EXIT__\".\n    \"\"\"\n    say(\"-\" * 60 + f\"\\nVariable Selection [{session.get('dataset_short_name')}]:\\n\" + \"-\" * 60, logger=logger)\n    say(\"(Type 'back' to return to previous step or 'exit' to quit.)\", logger=logger)\n\n    while True:\n        raw = read_input(\"\\nEnter variable names (comma-separated): \", logger=logger)\n        if raw in (\"__EXIT__\", \"__BACK__\"):\n            return raw\n\n        variable_list = [ v.strip().lower().strip('\"').strip(\"'\") for v in raw.split(\",\") if v.strip() ]\n        if not variable_list:\n            say(\"\\nERROR: Please enter at least one variable name.\", logger=logger)\n            continue\n\n        all_valid = validate_variables(variable_list, variable_restrictions_list, restriction_allow)\n\n        if not all_valid:\n            if restriction_allow:\n                valid_vars = [v for v in variable_list if v in variable_restrictions_list]\n                invalid_vars = [v for v in variable_list if v not in variable_restrictions_list]\n                say(\"\\nERROR: Some variables are not recognized or not available for this dataset:\", logger=logger)\n                for iv in invalid_vars:\n                    say(f\"   - {iv}\", logger=logger)\n            else:\n                valid_vars = [v for v in variable_list if v not in variable_restrictions_list]\n                invalid_vars = [v for v in variable_list if v in variable_restrictions_list]\n                say(\"\\nERROR: The following variables are known to cause issues or are disallowed for this dataset:\", logger=logger)\n                for iv in invalid_vars:\n                    say(f\"   - {iv}\", logger=logger)\n                say(\"\\nPlease edit the invalid variable list for this dataset if you believe this is an error.\", logger=logger)\n\n            if valid_vars:\n                proceed = read_input(\n                    f\"\\nWould you like to proceed with only the valid variables ({', '.join(valid_vars)})? (y/n): \",\n                    logger=logger)\n                if proceed in NORMALIZATION_MAP[\"confirmation\"] and NORMALIZATION_MAP[\"confirmation\"][proceed] == \"yes\":\n                    say(\"\\nProceeding with valid subset.\\n\", logger=logger)\n                    session.set(\"variables\", valid_vars)\n                    return valid_vars\n                else:\n                    say(\"\\nLet's try again.\\n\", logger=logger)\n                    continue\n            else:\n                say(\"\\nERROR: No valid variables remain. Please try again.\\n\", logger=logger)\n                continue\n\n        say(f\"\\nYou selected [{len(variable_list)}] valid variables:\\n{', '.join(variable_list)}\", logger=logger)\n        confirm = read_input(\"\\nConfirm selection? (y/n): \", logger=logger)\n        if confirm in NORMALIZATION_MAP[\"confirmation\"] and NORMALIZATION_MAP[\"confirmation\"][confirm] == \"yes\":\n            session.set(\"variables\", variable_list)\n            return variable_list\n        else:\n            say(\"\\nLet's try again.\\n\", logger=logger)\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.prompts.prompt_skip_overwrite_files","title":"prompt_skip_overwrite_files","text":"<pre><code>prompt_skip_overwrite_files(session, *, logger=None)\n</code></pre> <p>Prompt user to choose skip/overwrite/case-by-case for existing files.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>Session state to store user choice.</p> required <code>logger</code> <code>Logger</code> <p>Logger for logging messages, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>One of \"overwrite_all\", \"skip_all\", \"case_by_case\"</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/prompts.py</code> <pre><code>def prompt_skip_overwrite_files(\n        session: SessionState,\n        *,\n        logger=None,\n        ) -&gt; str:\n    \"\"\"\n    Prompt user to choose skip/overwrite/case-by-case for existing files.\n\n    Parameters\n    ----------\n    session : SessionState\n        Session state to store user choice.\n    logger : logging.Logger, optional\n        Logger for logging messages, by default None.\n\n    Returns\n    -------\n    str\n        One of \"overwrite_all\", \"skip_all\", \"case_by_case\"\n    \"\"\"\n    say(\"\\n\" + \"-\" * 60 + f\"\\nExisting File Policy:\\n\" + \"-\" * 60, logger=logger)\n    say(\"\\t1. Overwrite all existing files\\n\\t2. Skip all existing files\\n\\t3. Case-by-case confirmation\", logger=logger)\n\n    while True:\n        choice = read_input(\"\\nEnter choice (1/2/3): \", logger=logger)\n        if choice in (\"__EXIT__\", \"__BACK__\"):\n            return choice\n        if choice in [\"1\", \"2\", \"3\"]:\n            break\n        say(\"Invalid input. Please enter 1, 2, or 3.\", logger=logger)\n\n    if choice == \"1\":\n        session.set(\"existing_file_action\", \"overwrite_all\")\n    elif choice == \"2\":\n        session.set(\"existing_file_action\", \"skip_all\")\n    else:\n        session.set(\"existing_file_action\", \"case_by_case\")\n\n    say(f\"You selected option [{choice}] - [{session.get('existing_file_action')}] \\n\", logger=logger)\n    return session.get(\"existing_file_action\")\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.prompts.prompt_parallelisation_settings","title":"prompt_parallelisation_settings","text":"<pre><code>prompt_parallelisation_settings(session, *, logger=None)\n</code></pre> <p>Ask user about parallel downloads and concurrency cap.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>Current session state to store parallelisation settings.</p> required <code>logger</code> <code>Logger</code> <p>Logger for logging messages, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict | str</code> <p>Dictionary with parallelisation settings, or control token \"BACK\" / \"EXIT\".</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/prompts.py</code> <pre><code>def prompt_parallelisation_settings(\n        session: SessionState,\n        *,\n        logger=None,\n        ) -&gt; dict | str:\n    \"\"\"\n    Ask user about parallel downloads and concurrency cap.\n\n    Parameters\n    ----------\n    session : SessionState\n        Current session state to store parallelisation settings.\n    logger : logging.Logger, optional\n        Logger for logging messages, by default None.\n\n\n    Returns\n    -------\n    dict | str\n        Dictionary with parallelisation settings, or control token \"__BACK__\" / \"__EXIT__\".\n    \"\"\"\n    say(\"-\"*60 + \"\\nParallelisation Settings:\\n\" + '-'*60, logger=logger)\n    say(\"You can enable multiple parallel downloads to speed up retrieval.\\nNote: The CDS API may throttle or reject requests if too many are opened concurrently.\",\n        logger=logger)\n\n    while True:\n        raw = read_input(\"\\nEnable parallel downloads? (y/n) [default y]: \", logger=logger)\n        if raw in (\"__EXIT__\", \"__BACK__\"):\n            return raw\n\n        if raw == \"\":\n            user_choice = \"yes\"\n        elif raw in NORMALIZATION_MAP[\"confirmation\"]:\n            user_choice = NORMALIZATION_MAP[\"confirmation\"][raw]\n        else:\n            say(\"\\nInvalid input. Please enter 'y' or 'n'.\", logger=logger)\n            continue\n\n        if user_choice == \"no\":\n            settings = {\"enabled\": False, \"max_concurrent\": 1}\n            session.set(\"parallel_settings\", settings)\n            say(\"\\nYou have disabled parallel downloads (single-threaded mode).\", logger=logger)\n            return settings\n\n        while True:\n            mc = read_input(\"\\nEnter the maximum number of concurrent downloads you would like to allow (integer \u2265 2) [default 2]: \",\n                            logger=logger)\n            if mc in (\"__EXIT__\", \"__BACK__\"):\n                return mc\n            try:\n                mc = int(mc) if mc else 2\n            except ValueError:\n                say(\"\\nERROR: Please enter a valid integer.\", logger=logger)\n                continue\n            if mc &lt; 2:\n                say(\"\\nERROR: Parallel mode requires at least 2 concurrent downloads.\", logger=logger)\n                continue\n            if mc &gt; 2:\n                say(\"\\nWARNING: Using more than 2 parallel CDS downloads may cause throttling or request failures.\", logger=logger)\n                while True:\n                    confirm = read_input(\"\\nDo you still want to continue? (y/n): \", logger=logger)\n                    if confirm in (\"__EXIT__\", \"__BACK__\"):\n                        return confirm\n                    if confirm in NORMALIZATION_MAP[\"confirmation\"]:\n                        confirm_choice = NORMALIZATION_MAP[\"confirmation\"][confirm]\n                        break\n                    say(\"\\nInvalid input. Please enter 'y' or 'n'.\", logger=logger)\n                if confirm_choice == \"no\":\n                    continue\n\n            settings = {\"enabled\": True, \"max_concurrent\": mc}\n            session.set(\"parallel_settings\", settings)\n            say(f\"\\nYou have enabled parallel downloads with a maximum of [{mc}] concurrent downloads.\\n\", logger=logger)\n            return settings\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.prompts.prompt_retry_settings","title":"prompt_retry_settings","text":"<pre><code>prompt_retry_settings(\n    session,\n    default_retries=6,\n    default_delay=15,\n    *,\n    logger=None\n)\n</code></pre> <p>Ask user for retry limits.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>Current session state to store retry settings.</p> required <code>default_retries</code> <code>int</code> <p>Default number of retry attempts (default = 6).</p> <code>6</code> <code>default_delay</code> <code>int</code> <p>Default delay (in seconds) between retries (default = 15).</p> <code>15</code> <code>logger</code> <code>Logger</code> <p>Logger for logging messages, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict | str</code> <p>Dictionary with 'max_retries' and 'retry_delay_sec', or control token \"BACK\" / \"EXIT\".</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/prompts.py</code> <pre><code>def prompt_retry_settings(\n        session: SessionState,\n        default_retries: int = 6,\n        default_delay: int = 15,\n        *,\n        logger=None,\n        ) -&gt; dict | str:\n    \"\"\"\n    Ask user for retry limits.\n\n    Parameters\n    ----------\n    session : SessionState\n        Current session state to store retry settings.\n    default_retries : int\n        Default number of retry attempts (default = 6).\n    default_delay : int\n        Default delay (in seconds) between retries (default = 15).\n    logger : logging.Logger, optional\n        Logger for logging messages, by default None.\n\n    Returns\n    -------\n    dict | str\n        Dictionary with 'max_retries' and 'retry_delay_sec', or control token \"__BACK__\" / \"__EXIT__\".\n    \"\"\"\n    say(\"-\"*60 + \"\\nRetry Settings:\\n\" + '-'*60, logger=logger)\n    say(\"These settings control how the program handles failed download attempts.\", logger=logger)\n    say(f\"Default values \u2192 Retries: {default_retries}, Delay: {default_delay}s\", logger=logger)\n\n    while True:\n        user_input = read_input(\"\\nWould you like to use the default retry settings? (y/n) [default y]: \",\n                                logger=logger)\n        if user_input in (\"__EXIT__\", \"__BACK__\"):\n            return user_input\n\n        if user_input == \"\":\n            normalized = \"yes\"\n        elif user_input in NORMALIZATION_MAP[\"confirmation\"]:\n            normalized = NORMALIZATION_MAP[\"confirmation\"][user_input]\n        else:\n            say(\"\\nERROR: Invalid input. Please enter 'y' or 'n'.\", logger=logger)\n            continue\n\n        if normalized == \"yes\":\n            settings = {\"max_retries\": default_retries, \"retry_delay_sec\": default_delay}\n            session.set(\"retry_settings\", settings)\n            say(f\"\\nUsing default retry settings of max_retries: [{default_retries}], retry_delay_sec: [{default_delay}]\", logger=logger)\n            return settings\n\n        while True:\n            max_retries_raw = read_input(\"\\nEnter maximum number of retries (integer \u2265 0): \", logger=logger)\n            if max_retries_raw in (\"__EXIT__\", \"__BACK__\"):\n                return max_retries_raw\n            delay_raw = read_input(\"\\nEnter delay between retries (seconds, integer \u2265 0): \", logger=logger)\n            if delay_raw in (\"__EXIT__\", \"__BACK__\"):\n                return delay_raw\n\n            try:\n                max_retries = int(max_retries_raw) if max_retries_raw else default_retries\n                retry_delay_sec = int(delay_raw) if delay_raw else default_delay\n                if max_retries &lt; 0 or retry_delay_sec &lt; 0:\n                    say(\"\\nERROR: Values must be non-negative integers.\", logger=logger)\n                    continue\n                settings = {\"max_retries\": max_retries, \"retry_delay_sec\": retry_delay_sec}\n                session.set(\"retry_settings\", settings)\n                say(f\"\\nYou set max_retries=[{max_retries}], retry_delay_sec=[{retry_delay_sec}].\", logger=logger)\n                return settings\n            except ValueError:\n                say(\"Please enter valid integer values for retries and delay.\", logger=logger)\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.io.prompts.prompt_continue_confirmation","title":"prompt_continue_confirmation","text":"<pre><code>prompt_continue_confirmation(session, *, logger=None)\n</code></pre> <p>Display a formatted download summary and confirm before starting downloads.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>session state to summarise.</p> required <code>logger</code> <code>Logger</code> <p>Logger for logging messages, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool | str</code> <p>True if user confirms, False if user declines, or control token \"BACK\" / \"EXIT\".</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/io/prompts.py</code> <pre><code>def prompt_continue_confirmation(\n        session: SessionState,\n        *,\n        logger=None,\n        ) -&gt; bool | str:\n    \"\"\"\n    Display a formatted download summary and confirm before starting downloads.\n\n    Parameters\n    ----------\n    session : SessionState\n        session state to summarise.\n    logger : logging.Logger, optional\n        Logger for logging messages, by default None.\n\n\n    Returns\n    -------\n    bool | str\n        True if user confirms, False if user declines,\n        or control token \"__BACK__\" / \"__EXIT__\".\n    \"\"\"\n    text = session.summary()\n    say(text, logger=logger)\n    while True:\n        user_input = read_input(\"\\nProceed with download? (y/n): \", logger=logger)\n        if user_input in (\"__EXIT__\", \"__BACK__\"):\n            return user_input\n        if user_input in NORMALIZATION_MAP[\"confirmation\"]:\n            choice = NORMALIZATION_MAP[\"confirmation\"][user_input]\n            if choice == \"yes\":\n                say(\"\\nProceeding with download...\\n\", logger=logger)\n                return True\n            say(\"\\nDownload cancelled by user.\", logger=logger)\n            return False\n        say(\"\\nERROR: Invalid input. Please enter 'y' or 'n'.\", logger=logger)\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.sources.cds_era5","title":"weather_data_retrieval.sources.cds_era5","text":""},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.sources.cds_era5.prepare_cds_download","title":"prepare_cds_download","text":"<pre><code>prepare_cds_download(\n    session,\n    filename_base,\n    year,\n    month,\n    *,\n    logger,\n    echo_console,\n    allow_prompts,\n    dataset_config_mapping=CDS_DATASETS\n)\n</code></pre> <p>Check if a monthly ERA5 file already exists and decide whether to download.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>Session containing user configuration.</p> required <code>filename_base</code> <code>str</code> <p>Base name for the file.</p> required <code>year</code> <code>int</code> <p>Year of the data to download.</p> required <code>month</code> <code>int</code> <p>Month of the data to download.</p> required <code>logger</code> <code>Logger</code> <p>Logger for logging messages.</p> required <code>echo_console</code> <code>bool</code> <p>Whether to echo prompts to console.</p> required <code>allow_prompts</code> <code>bool</code> <p>Whether to allow interactive prompts.</p> required <code>dataset_config_mapping</code> <code>dict</code> <p>Mapping of dataset short names to their configurations.</p> <code>CDS_DATASETS</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>(download: bool, save_path: str)</code> <p>download: Whether to perform the download. save_path: Full path for the target file.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/sources/cds_era5.py</code> <pre><code>def prepare_cds_download(\n    session: SessionState,\n    filename_base: str,\n    year: int,\n    month: int,\n    *,\n    logger,\n    echo_console: bool,\n    allow_prompts: bool,\n    dataset_config_mapping: dict = CDS_DATASETS,\n) -&gt; Tuple[bool, str]:\n    \"\"\"\n    Check if a monthly ERA5 file already exists and decide whether to download.\n\n    Parameters\n    ----------\n    session : SessionState\n        Session containing user configuration.\n    filename_base : str\n        Base name for the file.\n    year : int\n        Year of the data to download.\n    month : int\n        Month of the data to download.\n    logger : logging.Logger, optional\n        Logger for logging messages.\n    echo_console : bool\n        Whether to echo prompts to console.\n    allow_prompts : bool\n        Whether to allow interactive prompts.\n    dataset_config_mapping : dict, optional\n        Mapping of dataset short names to their configurations.\n\n    Returns\n    -------\n    tuple: (download: bool, save_path: str)\n        download: Whether to perform the download.\n        save_path: Full path for the target file.\n    \"\"\"\n    cfg = get_cds_dataset_config(session, dataset_config_mapping)\n    data_file_format = cfg.get(\"data_download_format\", \"grib\")\n\n    # Resolve base save directory\n    raw_save_dir = session.get(\"save_dir\") or data_dir(create=True)\n    save_dir = resolve_under(data_dir(create=True), raw_save_dir)\n\n    # Construct canonical save path\n    save_path = expected_save_path(save_dir, filename_base, year, month, data_file_format)\n    policy = session.get(\"existing_file_action\") or \"case_by_case\"\n    download = True\n\n    if save_path.exists():\n        if policy == \"skip_all\":\n            log_msg(f\"Skipping existing file for {year}-{month:02d}: {save_path}\", logger, echo_console=echo_console)\n            download = False\n\n        elif policy == \"overwrite_all\":\n            log_msg(f\"Overwriting existing file for {year}-{month:02d}: {save_path}\", logger, echo_console=echo_console)\n\n        elif policy == \"case_by_case\":\n            if not allow_prompts:\n                raise ValueError(\n                    \"existing_file_action='case_by_case' requires interactive mode. \"\n                    \"Use 'skip_all' or 'overwrite_all' for automatic runs.\"\n                )\n            while True:\n                user_input = read_input(\n                    f\"\\nFile already exists for {year}-{month:02d}: {save_path}\\n\"\n                    \"Do you want to overwrite it? (y/n): \",\n                    logger=logger,\n                )\n                if user_input in NORMALIZATION_MAP[\"confirmation\"]:\n                    yn = NORMALIZATION_MAP[\"confirmation\"][user_input]\n                    if yn == \"yes\":\n                        log_msg(\n                            f\"Overwriting existing file for {year}-{month:02d}: {save_path}\",\n                            logger, echo_console=echo_console\n                        )\n                        download = True\n                    else:\n                        log_msg(\n                            f\"Skipping existing file for {year}-{month:02d}: {save_path}\",\n                            logger, echo_console=echo_console\n                        )\n                        download = False\n                    break\n                log_msg(\"Invalid input. Please enter 'y' or 'n'.\", logger, echo_console=echo_console)\n        else:\n            log_msg(\n                f\"Unknown existing_file_action policy '{policy}'; defaulting to 'skip_all'.\",\n                logger, level=\"warning\", echo_console=echo_console\n            )\n            session.set(\"existing_file_action\", \"skip_all\")\n            download = False\n\n    return download, save_path\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.sources.cds_era5.execute_cds_download","title":"execute_cds_download","text":"<pre><code>execute_cds_download(\n    session,\n    save_path,\n    year,\n    month,\n    *,\n    logger,\n    echo_console,\n    dataset_config_mapping=CDS_DATASETS\n)\n</code></pre> <p>Execute a single ERA5 monthly download with retry logic.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>Session state containing the authenticated CDS API client.</p> required <code>save_path</code> <code>str</code> <p>Full path to save the downloaded file.</p> required <code>year</code> <code>int</code> <p>Year of the data to download.</p> required <code>month</code> <code>int</code> <p>Month of the data to download.</p> required <code>logger</code> <code>Logger</code> <p>Logger for logging messages.</p> required <code>echo_console</code> <code>bool</code> <p>Whether to echo prompts to console.</p> required <code>dataset_config_mapping</code> <code>dict</code> <p>Mapping of dataset short names to their configurations.</p> <code>CDS_DATASETS</code> <p>Returns:</p> Type Description <code>(year, month, status): tuple</code> <p>status = \"success\" | \"failed\"</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/sources/cds_era5.py</code> <pre><code>def execute_cds_download(\n        session: SessionState,\n        save_path: str,\n        year: int,\n        month: int,\n        *,\n        logger,\n        echo_console: bool,\n        dataset_config_mapping: dict = CDS_DATASETS,\n        ) -&gt; tuple[int, int, str]:\n    \"\"\"\n    Execute a single ERA5 monthly download with retry logic.\n\n    Parameters\n    ----------\n    session : SessionState\n        Session state containing the authenticated CDS API client.\n    save_path : str\n        Full path to save the downloaded file.\n    year : int\n        Year of the data to download.\n    month : int\n        Month of the data to download.\n    logger : logging.Logger, optional\n        Logger for logging messages.\n    echo_console : bool\n        Whether to echo prompts to console.\n    dataset_config_mapping : dict, optional\n        Mapping of dataset short names to their configurations.\n\n    Returns\n    -------\n    (year, month, status): tuple\n        status = \"success\" | \"failed\"\n    \"\"\"\n    # Pull config fresh from CDS_DATASETS\n    cfg = get_cds_dataset_config(session, dataset_config_mapping=dataset_config_mapping)\n    dataset_product_name = cfg[\"dataset_product_name\"]\n    product_type = cfg[\"product_type\"]\n    data_download_format = cfg.get(\"data_download_format\", \"grib\")\n    times = cfg.get(\"default_times\", [f\"{h:02d}:00\" for h in range(24)])\n\n    variables = session.get(\"variables\")\n    grid_area = session.get(\"region_bounds\")\n    cds_client_session = session.get(\"session_client\")\n    if cds_client_session is None:\n        raise ValueError(\"CDS client not initialized in session\")\n\n    retry_conf = session.get(\"retry_settings\") or {\"max_retries\": 6, \"retry_delay_sec\": 15}\n    max_retries = retry_conf[\"max_retries\"]\n    retry_delay_sec = retry_conf[\"retry_delay_sec\"]\n    days = month_days(year, month)\n\n    month_start = time.time()\n    for attempt in range(1, max_retries + 1):\n        try:\n            log_msg(f\"\\tAttempt {attempt} of {max_retries} for {year}-{month:02d}...\", logger, echo_console=echo_console)\n            cds_client_session.retrieve(\n                dataset_product_name,\n                {\n                    \"product_type\": [product_type],\n                    \"variable\": variables,\n                    \"year\": str(year),\n                    \"month\": [f\"{month:02d}\"],\n                    \"day\": days,\n                    \"time\": times,\n                    \"area\": grid_area,\n                    \"format\": data_download_format,\n                },\n                str(save_path),\n            )\n            elapsed = time.time() - month_start\n            log_msg(f\"SUCCESS: {year}-{month:02d} in {format_duration(elapsed)}\", logger, echo_console=echo_console)\n            return (year, month, \"success\")\n\n        except Exception as e:\n            log_msg(f\"WARNING: Attempt {attempt} failed for {year}-{month:02d}: {e}\", logger, level=\"warning\", echo_console=echo_console)\n            if attempt &lt; max_retries:\n                log_msg(f\"\\tWaiting {retry_delay_sec} seconds before retrying...\", logger, echo_console=echo_console)\n                time.sleep(retry_delay_sec)\n                try:\n                    api_url, api_key = session.get(\"api_url\"), session.get(\"api_key\")\n                    creds_dict = {\"url\": api_url, \"key\": api_key}\n                    cds_client_session_new = ensure_cds_connection(cds_client_session, creds_dict)\n                    if cds_client_session_new is None:\n                        raise RuntimeError(\"Re-authentication returned None client.\")\n                    session.set(\"session_client\", cds_client_session_new)\n                    cds_client_session = cds_client_session_new\n                    log_msg(\"\\tRe-authenticated CDS API client.\", logger, echo_console=echo_console)\n                except Exception as auth_e:\n                    log_msg(f\"\\tRe-authentication failed: {auth_e}\", logger, level=\"warning\", echo_console=echo_console)\n            else:\n                log_msg(f\"FAILURE: all {max_retries} attempts failed for {year}-{month:02d}.\", logger, level=\"error\", echo_console=echo_console)\n                return (year, month, \"failed\")\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.sources.cds_era5.download_cds_month","title":"download_cds_month","text":"<pre><code>download_cds_month(\n    session,\n    filename_base,\n    year,\n    month,\n    *,\n    logger,\n    echo_console,\n    allow_prompts,\n    successful_downloads,\n    failed_downloads,\n    skipped_downloads\n)\n</code></pre> <p>Orchestrate ERA5 monthly download: handle file checks, then execute download.</p> <p>Parameters:</p> Name Type Description Default <code>Combines</code> required <p>Returns:</p> Type Description <code>(year, month, status): tuple</code> <p>status = \"success\" | \"failed\" | \"skipped\"</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/sources/cds_era5.py</code> <pre><code>def download_cds_month(\n    session: SessionState,\n    filename_base: str,\n    year: int,\n    month: int,\n    *,\n    logger,\n    echo_console: bool,\n    allow_prompts: bool,\n    successful_downloads: list,\n    failed_downloads: list,\n    skipped_downloads: list,\n    ) -&gt; Tuple[int, int, str]:\n    \"\"\"\n    Orchestrate ERA5 monthly download: handle file checks, then execute download.\n\n    Parameters\n    ----------\n    Combines parameters from `prepare_download` and `execute_download`.\n\n    Returns\n    -------\n    (year, month, status): tuple\n        status = \"success\" | \"failed\" | \"skipped\"\n\n    \"\"\"\n\n    proceed, save_path = prepare_cds_download(\n        session=session,\n        filename_base=filename_base,\n        year=year,\n        month=month,\n        logger=logger,\n        echo_console=echo_console,\n        allow_prompts=allow_prompts,\n        dataset_config_mapping=CDS_DATASETS,\n    )\n\n    if not proceed:\n        skipped_downloads.append((year, month))\n        return (year, month, \"skipped\")\n\n    y, m, status = execute_cds_download(\n        session=session,\n        save_path=save_path,\n        year=year,\n        month=month,\n        logger=logger,\n        echo_console=echo_console,\n        dataset_config_mapping=CDS_DATASETS,\n    )\n\n    if status == \"success\":\n        successful_downloads.append((y, m))\n    else:\n        failed_downloads.append((y, m))\n    return (y, m, status)\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.sources.cds_era5.plan_cds_months","title":"plan_cds_months","text":"<pre><code>plan_cds_months(\n    session,\n    filename_base,\n    *,\n    logger,\n    echo_console,\n    allow_prompts\n)\n</code></pre> <p>Build the list of months to download and which are being skipped due to existing files.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>Session containing user configuration.</p> required <code>filename_base</code> <code>str</code> <p>Base filename (without date or extension).</p> required <code>logger</code> <code>Logger</code> <p>Logger for logging messages.</p> required <code>echo_console</code> <code>bool</code> <p>Whether to echo prompts to console.</p> required <code>allow_prompts</code> <code>bool</code> <p>Whether to allow interactive prompts.</p> required <p>Returns:</p> Type Description <code>(months_to_download, months_skipped)</code> <ul> <li>months_to_download: list[(year, month)]</li> <li>months_skipped: list[(year, month, path)]</li> </ul> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/sources/cds_era5.py</code> <pre><code>def plan_cds_months(\n    session: SessionState,\n    filename_base: str,\n    *,\n    logger,\n    echo_console: bool,\n    allow_prompts: bool,\n) -&gt; Tuple[List[Tuple[int, int]], List[Tuple[int, int, Path]]]:\n    \"\"\"\n    Build the list of months to download and which are being skipped due to existing files.\n\n    Parameters\n    ----------\n    session : SessionState\n        Session containing user configuration.\n    filename_base : str\n        Base filename (without date or extension).\n    logger : logging.Logger, optional\n        Logger for logging messages.\n    echo_console : bool\n        Whether to echo prompts to console.\n    allow_prompts : bool\n        Whether to allow interactive prompts.\n\n    Returns\n    -------\n    (months_to_download, months_skipped)\n      - months_to_download: list[(year, month)]\n      - months_skipped: list[(year, month, path)]\n    \"\"\"\n    policy = validate_existing_file_action(session, allow_prompts=allow_prompts, logger=logger, echo_console=echo_console)\n\n    start_date = session.get(\"start_date\")\n    end_date = session.get(\"end_date\")\n    save_dir = Path(session.get(\"save_dir\"))\n\n    s = datetime.strptime(start_date, \"%Y-%m-%d\")\n    e = datetime.strptime(end_date, \"%Y-%m-%d\")\n\n    # Build month list\n    months: List[Tuple[int, int]] = []\n    cur = datetime(s.year, s.month, 1)\n    while cur &lt;= e:\n        months.append((cur.year, cur.month))\n        cur = datetime(cur.year + 1, 1, 1) if cur.month == 12 else datetime(cur.year, cur.month + 1, 1)\n\n    months_to_download: List[Tuple[int, int]] = []\n    months_skipped: List[Tuple[int, int, Path]] = []\n\n    for (y, m) in months:\n        existing = find_existing_month_file(save_dir, filename_base, y, m)\n        if existing is None:\n            months_to_download.append((y, m))\n            continue\n\n        if policy == \"skip_all\":\n            months_skipped.append((y, m, existing))\n        elif policy == \"overwrite_all\":\n            months_skipped.append((y, m, existing))\n            months_to_download.append((y, m))\n        else:\n            # interactive 'case_by_case'\n            while True:\n                ans = read_input(\n                    f\"\\nFile already exists for {y}-{m:02d}: {existing}\\n\"\n                    f\"Overwrite this one? (y/n): \",\n                    logger=logger,\n                )\n                if ans in NORMALIZATION_MAP[\"confirmation\"]:\n                    yn = NORMALIZATION_MAP[\"confirmation\"][ans]\n                    if yn == \"yes\":\n                        months_to_download.append((y, m))\n                    else:\n                        months_skipped.append((y, m, existing))\n                    break\n                log_msg(\"Please enter 'y' or 'n'.\", logger, echo_console=echo_console)\n\n    # Report\n    log_msg(\"\\n===&gt; Checking for existing files...\\n\" + \"-\" * 60, logger, echo_console=echo_console)\n    if months_skipped:\n        for y, m, p in months_skipped[:5]:\n            log_msg(f\"  - {y}-{m:02d}: {p}\", logger, echo_console=echo_console)\n        if len(months_skipped) &gt; 5:\n            log_msg(f\"  ... and {len(months_skipped)-5} more.\", logger, echo_console=echo_console)\n    log_msg(f\"\\tFound [{len(months_skipped)}] existing month(s) out of [{len(months)}] requested month(s)\\n\", logger, echo_console=echo_console)\n    log_msg(f\"Policy for existing files: '{policy}'\", logger, echo_console=echo_console)\n\n    return months_to_download, months_skipped\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.sources.cds_era5.orchestrate_cds_downloads","title":"orchestrate_cds_downloads","text":"<pre><code>orchestrate_cds_downloads(\n    session,\n    filename_base,\n    successful_downloads,\n    failed_downloads,\n    skipped_downloads,\n    *,\n    logger,\n    echo_console,\n    allow_prompts,\n    dataset_config_mapping=CDS_DATASETS\n)\n</code></pre> <p>Handle and orchestrate ERA5 monthly downloads, supporting parallel or sequential execution.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>Session containing user configuration and authenticated client.</p> required <code>successful_downloads</code> <code>list</code> <p>Mutable list to collect (year, month) tuples for successful downloads.</p> required <code>failed_downloads</code> <code>list</code> <p>Mutable list to collect (year, month) tuples for failed downloads.</p> required <code>skipped_downloads</code> <code>list</code> <p>Mutable list to collect (year, month) tuples for skipped downloads.</p> required <code>logger</code> <code>Logger</code> <p>Logger for logging messages.</p> required <code>echo_console</code> <code>bool</code> <p>Whether to echo prompts to console.</p> required <code>allow_prompts</code> <code>bool</code> <p>Whether to allow interactive prompts.</p> required <code>dataset_config_mapping</code> <code>dict</code> <p>Mapping of dataset configurations, by default CDS_DATASETS.</p> <code>CDS_DATASETS</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/sources/cds_era5.py</code> <pre><code>def orchestrate_cds_downloads(\n        session: SessionState,\n        filename_base: str,\n        successful_downloads: list,\n        failed_downloads: list,\n        skipped_downloads: list,\n        *,\n        logger,\n        echo_console: bool,\n        allow_prompts: bool,\n        dataset_config_mapping: dict = CDS_DATASETS,\n        ) -&gt; None:\n    \"\"\"\n    Handle and orchestrate ERA5 monthly downloads, supporting parallel or sequential execution.\n\n    Parameters\n    ----------\n    session : SessionState\n        Session containing user configuration and authenticated client.\n    successful_downloads : list\n        Mutable list to collect (year, month) tuples for successful downloads.\n    failed_downloads : list\n        Mutable list to collect (year, month) tuples for failed downloads.\n    skipped_downloads : list\n        Mutable list to collect (year, month) tuples for skipped downloads.\n    logger : logging.Logger, optional\n        Logger for logging messages.\n    echo_console : bool\n        Whether to echo prompts to console.\n    allow_prompts : bool\n        Whether to allow interactive prompts.\n    dataset_config_mapping : dict, optional\n        Mapping of dataset configurations, by default CDS_DATASETS.\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    # (Optional) touch cfg to ensure dataset is valid; not stored, just validation side-effect\n    _ = get_cds_dataset_config(session, CDS_DATASETS)\n\n    months_to_download, months_skipped = plan_cds_months(\n        session=session,\n        filename_base=filename_base,\n        logger=logger,\n        echo_console=echo_console,\n        allow_prompts=allow_prompts,\n    )\n\n    for (y, m, _) in months_skipped:\n        skipped_downloads.append((y, m))\n\n    if not months_to_download:\n        log_msg(\"\\nNothing to download (all months skipped).\", logger, echo_console=echo_console)\n        return\n\n    parallel_conf = session.get(\"parallel_settings\") or {\"enabled\": False, \"max_concurrent\": 1}\n    t0 = perf_counter()\n    if parallel_conf.get(\"enabled\"):\n        max_workers = max(2, int(parallel_conf.get(\"max_concurrent\", 2)))\n        log_msg(f\"\\nParallelisation : Enabled -&gt; Beginning download with [{max_workers}] concurrent tasks...\\n\" + \"-\" * 60, logger, echo_console=echo_console)\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            tasks = [\n                executor.submit(\n                    download_cds_month,\n                    session=session,\n                    filename_base=filename_base,\n                    year=y,\n                    month=m,\n                    logger=logger,\n                    echo_console=echo_console,\n                    allow_prompts=allow_prompts,\n                    successful_downloads=successful_downloads,\n                    failed_downloads=failed_downloads,\n                    skipped_downloads=skipped_downloads,\n                )\n                for (y, m) in months_to_download\n            ]\n            for _ in as_completed(tasks):\n                pass\n    else:\n        log_msg(f\"\\nParallelisation : Disabled -&gt; Beginning download in sequential download mode ...\\n\" + \"-\" * 60, logger, echo_console=echo_console)\n        for (y, m) in months_to_download:\n            download_cds_month(\n                session=session,\n                filename_base=filename_base,\n                year=y,\n                month=m,\n                logger=logger,\n                echo_console=echo_console,\n                allow_prompts=allow_prompts,\n                successful_downloads=successful_downloads,\n                failed_downloads=failed_downloads,\n                skipped_downloads=skipped_downloads,\n            )\n    elapsed = perf_counter() - t0\n    log_msg(f\"\\nTotal download wall time: {format_duration(elapsed)}\",\n            logger, echo_console=echo_console)\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.sources.open_meteo","title":"weather_data_retrieval.sources.open_meteo","text":""},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.data_validation","title":"weather_data_retrieval.utils.data_validation","text":""},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.data_validation.normalize_input","title":"normalize_input","text":"<pre><code>normalize_input(value, category)\n</code></pre> <p>Normalize user input to canonical internal value as defined in NORMALIZATION_MAP.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The user input value to normalize.</p> required <code>category</code> <code>str</code> <p>The category of normalization (e.g., 'data_provider', 'dataset_short_name')</p> required <p>Returns:</p> Type Description <code>str</code> <p>The normalized value.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/data_validation.py</code> <pre><code>def normalize_input(\n        value: str,\n        category: str\n    ) -&gt; str:\n    \"\"\"\n    Normalize user input to canonical internal value as defined in NORMALIZATION_MAP.\n\n    Parameters\n    ----------\n    value : str\n        The user input value to normalize.\n    category : str\n        The category of normalization (e.g., 'data_provider', 'dataset_short_name')\n\n    Returns\n    -------\n    str\n        The normalized value.\n\n    \"\"\"\n    if not isinstance(value, str):\n        return value\n    v = value.strip().lower()\n    return NORMALIZATION_MAP.get(category, {}).get(v, value)\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.data_validation.format_duration","title":"format_duration","text":"<pre><code>format_duration(seconds)\n</code></pre> <p>Convert seconds to a nice Hh Mm Ss string (with decimal seconds).</p> <p>Parameters:</p> Name Type Description Default <code>seconds</code> <code>float</code> <p>Duration in seconds.</p> required <p>Returns:</p> Type Description <code>    str: Formatted duration string.</code> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/data_validation.py</code> <pre><code>def format_duration(seconds: float) -&gt; str:\n    \"\"\"\n    Convert seconds to a nice Hh Mm Ss string (with decimal seconds).\n\n    Parameters\n    ----------\n    seconds : float\n        Duration in seconds.\n\n    Returns\n    -------\n        str: Formatted duration string.\n    \"\"\"\n    seconds = max(0, seconds)\n\n    hours = int(seconds // 3600)\n    days = int(hours // 24)\n    minutes = int((seconds % 3600) // 60)\n    secs = seconds % 60  # keep remainder as float\n\n    if days &gt; 0:\n        hours = hours % 24\n        return f\"{days}d {hours}h {minutes}m {secs:.2f}s\"\n    if hours &gt; 0:\n        return f\"{hours}h {minutes}m {secs:.2f}s\"\n    elif minutes &gt; 0:\n        return f\"{minutes}m {secs:.2f}s\"\n    else:\n        return f\"{secs:.5f}s\"\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.data_validation.format_coordinates_nwse","title":"format_coordinates_nwse","text":"<pre><code>format_coordinates_nwse(boundaries)\n</code></pre> <p>Extracts and formats coordinates as integers in N-W-S-E order Used for compact representation in filenames.</p> <p>Parameters:</p> Name Type Description Default <code>boundaries</code> <code>list</code> <p>List of boundaries in the order [north, west, south, east]</p> required <p>Returns:</p> Type Description <code>    str: Formatted string in the format 'N{north}W{west}S{south}E{east}'</code> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/data_validation.py</code> <pre><code>def format_coordinates_nwse(boundaries: List[float]) -&gt; str:\n\n    \"\"\"\n    Extracts and formats coordinates as integers in N-W-S-E order\n    Used for compact representation in filenames.\n\n    Parameters\n    ----------\n    boundaries : list\n        List of boundaries in the order [north, west, south, east]\n    Returns\n    -------\n        str: Formatted string in the format 'N{north}W{west}S{south}E{east}'\n    \"\"\"\n    n, w, s, e = boundaries\n    # compact string for filenames\n    return f\"N{int(n)}W{int(w)}S{int(s)}E{int(e)}\"\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.data_validation.month_days","title":"month_days","text":"<pre><code>month_days(year, month)\n</code></pre> <p>Get list of days in a month formatted as two-digit strings.</p> <p>Parameters:</p> Name Type Description Default <code>year</code> <code>int</code> <p>Year of interest.</p> required <code>month</code> <code>int</code> <p>Month of interest.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of days in the month as two-digit strings.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/data_validation.py</code> <pre><code>def month_days(\n        year: int,\n        month: int\n        ) -&gt; List[str]:\n    \"\"\"\n    Get list of days in a month formatted as two-digit strings.\n\n    Parameters\n    ----------\n    year : int\n        Year of interest.\n    month : int\n        Month of interest.\n\n    Returns\n    -------\n    List[str]\n        List of days in the month as two-digit strings.\n    \"\"\"\n    last = calendar.monthrange(year, month)[1]\n    return [f\"{d:02d}\" for d in range(1, last + 1)]\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.data_validation.validate_data_provider","title":"validate_data_provider","text":"<pre><code>validate_data_provider(provider)\n</code></pre> <p>Ensure dataprovider is recognized and implemented.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Name of the data provider.</p> required <p>Returns:</p> Type Description <code>    bool: True if valid, False otherwise.</code> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/data_validation.py</code> <pre><code>def validate_data_provider(provider: str) -&gt; bool:\n    \"\"\"\n    Ensure dataprovider is recognized and implemented.\n\n    Parameters\n    ----------\n    provider : str\n        Name of the data provider.\n    Returns\n    -------\n        bool: True if valid, False otherwise.\n    \"\"\"\n    if provider not in (\"cds\", \"open-meteo\"):\n        return False\n    return True\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.data_validation.validate_dataset_short_name","title":"validate_dataset_short_name","text":"<pre><code>validate_dataset_short_name(dataset_short_name, provider)\n</code></pre> <p>Check dataset compatibility with provider.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_short_name</code> <code>str</code> <p>Dataset short name.</p> required <code>provider</code> <code>str</code> <p>Name of the data provider.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if valid, False otherwise.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/data_validation.py</code> <pre><code>def validate_dataset_short_name(\n        dataset_short_name: str,\n        provider: str\n        ) -&gt; bool:\n    \"\"\"\n    Check dataset compatibility with provider.\n\n    Parameters\n    ----------\n    dataset_short_name : str\n        Dataset short name.\n    provider : str\n        Name of the data provider.\n\n    Returns\n    -------\n    bool\n        True if valid, False otherwise.\n    \"\"\"\n    if provider == \"cds\":\n        return dataset_short_name in CDS_DATASETS\n    if provider == \"open-meteo\":\n        raise NotImplementedError(\"Open-Meteo dataset validation not yet implemented.\")\n    log_msg(f\"Warning: Unknown provider '{provider}'.\")\n    return False\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.data_validation.validate_cds_api_key","title":"validate_cds_api_key","text":"<pre><code>validate_cds_api_key(\n    url, key, *, logger=None, echo_console=False\n)\n</code></pre> <p>Validate CDS API credentials by attempting to initialize a cdsapi.Client.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>CDS API URL.</p> required <code>key</code> <code>str</code> <p>CDS API key.</p> required <code>logger</code> <code>Logger</code> <p>Logger for logging messages, by default None.</p> <code>None</code> <code>echo_console</code> <code>bool</code> <p>Whether to echo messages to console, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Client | None</code> <p>Authenticated client if successful, otherwise None.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/data_validation.py</code> <pre><code>def validate_cds_api_key(\n        url: str,\n        key: str,\n        *,\n        logger=None,\n        echo_console: bool = False\n        ) -&gt; cdsapi.Client | None:\n    \"\"\"\n    Validate CDS API credentials by attempting to initialize a cdsapi.Client.\n\n    Parameters\n    ----------\n    url : str\n        CDS API URL.\n    key : str\n        CDS API key.\n    logger : logging.Logger, optional\n        Logger for logging messages, by default None.\n    echo_console : bool, optional\n        Whether to echo messages to console, by default False.\n\n    Returns\n    -------\n    cdsapi.Client | None\n        Authenticated client if successful, otherwise None.\n    \"\"\"\n\n    if logger:\n        log_msg(\"Testing CDS API connection with provided credentials...\", logger, level=\"info\")\n\n    # 1) Initialize client\n    try:\n        client = cdsapi.Client(url=url, key=key, quiet=True, timeout=30)\n    except Exception as e:\n        if logger:\n            log_msg(f\"\\tFailed to initialize CDS client: {e}\", logger, level=\"warning\")\n        return None\n\n    # 2) Probe using the predefined payload (normalize keys minimally)\n    #    Expecting a dict like: test_payload = {\"dataset\": \"...\", \"request\": {...}}\n    try:\n        dataset = test_payload[\"dataset\"]\n        request = dict(test_payload[\"request\"])  # shallow copy so we can tweak keys safely\n\n        # product_type must be a string\n        if isinstance(request.get(\"product_type\"), list) and request[\"product_type\"]:\n            request[\"product_type\"] = request[\"product_type\"][0]\n\n        # -------------------------------------------------------------------\n        tmp_path = Path(tempfile.gettempdir()) / f\"cds_probe_{os.getpid()}_{int(datetime.now().timestamp())}.grib\"\n\n        if logger:\n            log_msg(\"\\tProbing ERA5 permissions with a minimal retrieve...\", logger)\n            request_items = \"\"\n            for k, v in request.items():\n                request_items += f\"\\n\\t   {k}: {v}\"\n            log_msg(f\"\\tRequest details: dataset='{dataset}' {request_items}\", logger)\n\n        client.retrieve(dataset, request, target=str(tmp_path))\n\n        if logger:\n            log_msg(\"\\tPermission probe succeeded.\\n\", logger)\n        return client\n\n    except Exception as e:\n        msg = str(e)\n        if logger:\n            log_msg(f\"\\tAuthentication/probe failed: {msg}\\n\", logger, level=\"warning\")\n            if \"401\" in msg or \"Unauthorized\" in msg or \"operation not allowed\" in msg:\n                log_msg(\n                    f\"\\tCDS returned 401/Unauthorized. This usually means you haven\u2019t accepted the \"\n                    f\"licence for '{test_payload.get('dataset','(unknown)')}'. Please log into the CDS website and accept it.\\n\",\n                    logger,\n                    level=\"warning\",\n                )\n        return None\n    finally:\n        # Cleanup\n        try:\n            if 'tmp_path' in locals() and tmp_path.exists():\n                tmp_path.unlink()\n        except Exception:\n            pass\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.data_validation.validate_directory","title":"validate_directory","text":"<pre><code>validate_directory(path)\n</code></pre> <p>Check if path exists or can be created.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path to validate.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if path exists or was created successfully, False otherwise.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/data_validation.py</code> <pre><code>def validate_directory(path: str) -&gt; bool:\n    \"\"\"\n    Check if path exists or can be created.\n\n    Parameters\n    ----------\n    path : str\n        Directory path to validate.\n\n    Returns\n    -------\n    bool\n        True if path exists or was created successfully, False otherwise.\n    \"\"\"\n    p = Path(path)\n    if p.exists():\n        return True\n    try:\n        p.mkdir(parents=True, exist_ok=True)\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.data_validation.validate_date","title":"validate_date","text":"<pre><code>validate_date(value, allow_month_only=False)\n</code></pre> <p>Validate date format as YYYY-MM-DD or optionally YYYY-MM.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>Date string to validate.</p> required <code>allow_month_only</code> <code>bool</code> <p>If True, also accept YYYY-MM format, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if valid, False otherwise.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/data_validation.py</code> <pre><code>def validate_date(\n        value: str,\n        allow_month_only: bool = False\n        ) -&gt; bool:\n    \"\"\"\n    Validate date format as YYYY-MM-DD or optionally YYYY-MM.\n\n    Parameters\n    ----------\n    value : str\n        Date string to validate.\n    allow_month_only : bool, optional\n        If True, also accept YYYY-MM format, by default False.\n\n    Returns\n    -------\n    bool\n        True if valid, False otherwise.\n    \"\"\"\n    try:\n        # Try full date format first\n        datetime.strptime(value, \"%Y-%m-%d\")\n        return True\n    except ValueError:\n        if allow_month_only:\n            try:\n                # Try year-month format\n                datetime.strptime(value, \"%Y-%m\")\n                return True\n            except ValueError:\n                log_msg(f\"Invalid date format '{value}'. Expected YYYY-MM-DD or YYYY-MM.\\n\")\n                return False\n        return False\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.data_validation.parse_date_with_defaults","title":"parse_date_with_defaults","text":"<pre><code>parse_date_with_defaults(\n    date_str, default_to_month_end=False\n)\n</code></pre> <p>Parse date string and apply defaults for incomplete dates.</p> <p>Parameters:</p> Name Type Description Default <code>date_str</code> <code>str</code> <p>Date string in format YYYY-MM-DD or YYYY-MM.</p> required <code>default_to_month_end</code> <code>bool</code> <p>If True and date is YYYY-MM format, default to last day of month. If False and date is YYYY-MM format, default to first day of month. By default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[datetime, str]</code> <p>Tuple of (parsed datetime object, ISO format string YYYY-MM-DD)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If date string is invalid.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/data_validation.py</code> <pre><code>def parse_date_with_defaults(\n        date_str: str,\n        default_to_month_end: bool = False\n        ) -&gt; tuple[datetime, str]:\n    \"\"\"\n    Parse date string and apply defaults for incomplete dates.\n\n    Parameters\n    ----------\n    date_str : str\n        Date string in format YYYY-MM-DD or YYYY-MM.\n    default_to_month_end : bool, optional\n        If True and date is YYYY-MM format, default to last day of month.\n        If False and date is YYYY-MM format, default to first day of month.\n        By default False.\n\n    Returns\n    -------\n    tuple[datetime, str]\n        Tuple of (parsed datetime object, ISO format string YYYY-MM-DD)\n\n    Raises\n    ------\n    ValueError\n        If date string is invalid.\n    \"\"\"\n    if len(date_str) == 7:  # YYYY-MM format\n        year, month = date_str.split('-')\n        year, month = int(year), int(month)\n\n        if default_to_month_end:\n            # Get last day of month\n            last_day = calendar.monthrange(year, month)[1]\n            date_str_full = f\"{year}-{month:02d}-{last_day:02d}\"\n        else:\n            # Default to first day\n            date_str_full = f\"{year}-{month:02d}-01\"\n\n        dt = datetime.strptime(date_str_full, \"%Y-%m-%d\")\n        return dt, date_str_full\n\n    elif len(date_str) == 10:  # YYYY-MM-DD format\n        dt = datetime.strptime(date_str, \"%Y-%m-%d\")\n        return dt, date_str\n\n    else:\n        raise ValueError(f\"Invalid date format '{date_str}'. Expected YYYY-MM-DD or YYYY-MM.\")\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.data_validation.clamp_era5_available_end_date","title":"clamp_era5_available_end_date","text":"<pre><code>clamp_era5_available_end_date(end)\n</code></pre> <p>Clamp end date to ERA5 data availability boundary (8 days ago).</p> <p>Parameters:</p> Name Type Description Default <code>end</code> <code>datetime</code> <p>Desired end date.</p> required <p>Returns:</p> Name Type Description <code>    datetime: Clamped end date.</code> <code>NOTES</code> <code>ERA5 data is available up to 8 days prior to the current date.</code> <code>8-day lag is used to ensure data availability.</code> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/data_validation.py</code> <pre><code>def clamp_era5_available_end_date(end: datetime) -&gt; datetime:\n    \"\"\"\n    Clamp end date to ERA5 data availability boundary (8 days ago).\n\n    Parameters\n    ----------\n    end : datetime\n        Desired end date.\n\n    Returns\n    -------\n        datetime: Clamped end date.\n\n    NOTES: ERA5 data is available up to 8 days prior to the current date.\n    8-day lag is used to ensure data availability.\n\n    \"\"\"\n    EIGHT_DAY_LAG = 8\n    upper = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(days=EIGHT_DAY_LAG)\n    if end &gt; upper:\n        log_msg(f\"Adjusting end date from {end.date()} to data availability boundary {upper.date()} (\u2212{EIGHT_DAY_LAG} days).\")\n        return upper\n    return end\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.data_validation.validate_coordinates","title":"validate_coordinates","text":"<pre><code>validate_coordinates(north, west, south, east)\n</code></pre> <p>Ensure coordinates are within realistic bounds.</p> <p>Parameters:</p> Name Type Description Default <code>north</code> <code>int | float</code> <p>Northern latitude boundary.</p> required <code>west</code> <code>int | float</code> <p>Western longitude boundary.</p> required <code>south</code> <code>int | float</code> <p>Southern latitude boundary.</p> required <code>east</code> <code>int | float</code> <p>Eastern longitude boundary.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if coordinates are valid, False otherwise.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/data_validation.py</code> <pre><code>def validate_coordinates(\n        north: int | float,\n        west: int | float,\n        south: int | float,\n        east: int | float,\n        ) -&gt; bool:\n    \"\"\"\n    Ensure coordinates are within realistic bounds.\n\n    Parameters\n    ----------\n    north : int | float\n        Northern latitude boundary.\n    west : int | float\n        Western longitude boundary.\n    south : int | float\n        Southern latitude boundary.\n    east : int | float\n        Eastern longitude boundary.\n\n    Returns\n    -------\n    bool\n        True if coordinates are valid, False otherwise.\n    \"\"\"\n    return all([\n        -90 &lt;= south &lt;= 90,\n        -90 &lt;= north &lt;= 90,\n        -180 &lt;= west &lt;= 180,\n        -180 &lt;= east &lt;= 180,\n        north &gt; south\n    ])\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.data_validation.validate_variables","title":"validate_variables","text":"<pre><code>validate_variables(\n    variable_list,\n    variable_restrictions,\n    restriction_allow=False,\n)\n</code></pre> <p>Ensure user-specified variables are available for this dataset.</p> <p>Parameters:</p> Name Type Description Default <code>variable_list</code> <code>list[str]</code> <p>List of variable names to validate.</p> required <code>variable_restrictions</code> <code>list[str]</code> <p>List of variables that are either allowed or disallowed.</p> required <code>restriction_allow</code> <code>bool</code> <p>If True, variable_restrictions is an allowlist (i.e. in). If False, it's a denylist (i.e. not in)</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if all variables are valid, False otherwise.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/data_validation.py</code> <pre><code>def validate_variables(\n        variable_list: list[str],\n        variable_restrictions: list[str],\n        restriction_allow: bool = False\n        ) -&gt; bool:\n    \"\"\"\n    Ensure user-specified variables are available for this dataset.\n\n    Parameters\n    ----------\n    variable_list : list[str]\n        List of variable names to validate.\n    variable_restrictions : list[str]\n        List of variables that are either allowed or disallowed.\n    restriction_allow : bool\n        If True, variable_restrictions is an allowlist (i.e. in). If False, it's a denylist\n        (i.e. not in)\n\n    Returns\n    -------\n    bool\n        True if all variables are valid, False otherwise.\n    \"\"\"\n    if not variable_list:\n        return False\n\n    # Normalize for case consistency\n    variables = [v.lower().strip() for v in variable_list]\n    restrictions = [r.lower().strip() for r in variable_restrictions]\n\n    if restriction_allow:\n        return all(v in restrictions for v in variables)\n    else:\n        return all(v not in restrictions for v in variables)\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.data_validation.validate_existing_file_action","title":"validate_existing_file_action","text":"<pre><code>validate_existing_file_action(\n    session, *, allow_prompts, logger, echo_console=False\n)\n</code></pre> <p>Normalize existing_file_action for the current run-mode. - If 'case_by_case' is set but prompts are not allowed (automatic mode),   coerce to 'skip_all' and warn.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Any</code> <p>Current session state.</p> required <code>allow_prompts</code> <code>bool</code> <p>Whether prompts are allowed (i.e., interactive mode).</p> required <code>logger</code> <code>Logger</code> <p>Logger for logging messages.</p> required <code>echo_console</code> <code>bool</code> <p>Whether to echo messages to console.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Normalized existing_file_action policy.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/data_validation.py</code> <pre><code>def validate_existing_file_action(\n        session: Any,\n        *,\n        allow_prompts: bool,\n        logger,\n        echo_console: bool = False\n        ) -&gt; str:\n    \"\"\"\n    Normalize existing_file_action for the current run-mode.\n    - If 'case_by_case' is set but prompts are not allowed (automatic mode),\n      coerce to 'skip_all' and warn.\n\n    Parameters\n    ----------\n    session : Any\n        Current session state.\n    allow_prompts : bool\n        Whether prompts are allowed (i.e., interactive mode).\n    logger : logging.Logger\n        Logger for logging messages.\n    echo_console : bool\n        Whether to echo messages to console.\n\n    Returns\n    -------\n    str\n        Normalized existing_file_action policy.\n    \"\"\"\n    policy = session.get(\"existing_file_action\") or \"case_by_case\"\n    allowed = {\"overwrite_all\", \"skip_all\", \"case_by_case\"}\n\n    if policy not in allowed:\n        if allow_prompts:\n            # treat as case-by-case if we can prompt\n            log_msg(\n                f\"Unrecognized existing_file_action='{policy}'; treating as 'case_by_case' due to interactive mode.\",\n                logger, level=\"warning\", echo_console=echo_console\n            )\n            return \"case_by_case\"\n        else:\n            # automatic: force skip_all\n            log_msg(\n                f\"Unrecognized existing_file_action='{policy}'; coercing to 'skip_all' for automatic mode.\",\n                logger, level=\"warning\", echo_console=echo_console\n            )\n            session.set(\"existing_file_action\", \"skip_all\")\n            return \"skip_all\"\n\n    if policy == \"case_by_case\" and not allow_prompts:\n        log_msg(\n            \"existing_file_action='case_by_case' is not supported without prompts; \"\n            \"coercing to 'skip_all' for automatic mode.\",\n            logger, level=\"warning\", echo_console=echo_console\n        )\n        session.set(\"existing_file_action\", \"skip_all\")\n        return \"skip_all\"\n\n    return policy\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.data_validation.validate_config","title":"validate_config","text":"<pre><code>validate_config(\n    config,\n    *,\n    logger=None,\n    run_mode=\"automatic\",\n    echo_console=False,\n    live_auth_check=False\n)\n</code></pre> <p>Entry point. Validates common shape then dispatches to provider-specific validator.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary.</p> required <code>logger</code> <code>Logger</code> <p>Logger for logging messages, by default None.</p> <code>None</code> <code>run_mode</code> <code>str</code> <p>Run mode, either 'interactive' or 'automatic', by default \"automatic\".</p> <code>'automatic'</code> <code>echo_console</code> <code>bool</code> <p>Whether to echo messages to console, by default False.</p> <code>False</code> <code>live_auth_check</code> <code>bool</code> <p>Whether to perform live authentication checks (e.g., CDS API), by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/data_validation.py</code> <pre><code>def validate_config(\n        config: dict,\n        *,\n        logger=None,\n        run_mode: str = \"automatic\",\n        echo_console: bool = False,\n        live_auth_check: bool = False,\n        ) -&gt; None:\n    \"\"\"\n    Entry point. Validates common shape then dispatches to provider-specific validator.\n\n    Parameters\n    ----------\n    config : dict\n        Configuration dictionary.\n    logger : logging.Logger, optional\n        Logger for logging messages, by default None.\n    run_mode : str, optional\n        Run mode, either 'interactive' or 'automatic', by default \"automatic\".\n    echo_console : bool, optional\n        Whether to echo messages to console, by default False.\n    live_auth_check : bool, optional\n        Whether to perform live authentication checks (e.g., CDS API), by default False.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    _validate_common(config, logger=logger, run_mode=run_mode, echo_console=echo_console)\n\n    provider = config[\"data_provider\"]\n    if provider == \"cds\":\n        _validate_cds(config, logger=logger, run_mode=run_mode, echo_console=echo_console, live_auth_check=live_auth_check)\n    elif provider == \"open-meteo\":\n        _validate_open_meteo(config, logger=logger, run_mode=run_mode, echo_console=echo_console)\n    else:\n        raise ValueError(f\"Unsupported provider: {provider}\")\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.file_management","title":"weather_data_retrieval.utils.file_management","text":""},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.file_management.generate_filename_hash","title":"generate_filename_hash","text":"<pre><code>generate_filename_hash(\n    dataset_short_name, variables, boundaries\n)\n</code></pre> <p>Generate a unique hash for the download parameters that will be used to create the filename.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_short_name</code> <code>str</code> <p>The dataset short name (era5-world etc).</p> required <code>variables</code> <code>list[str]</code> <p>List of variable names.</p> required <code>boundaries</code> <code>list[float]</code> <p>List of boundaries [north, west, south, east].</p> required <p>Returns:</p> Type Description <code>    str: A unique hash string representing the download parameters.</code> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/file_management.py</code> <pre><code>def generate_filename_hash(\n        dataset_short_name: str,\n        variables: list[str],\n        boundaries: list[float]\n        ) -&gt; str:\n    \"\"\"\n    Generate a unique hash for the download parameters that will be used to create the filename.\n\n    Parameters\n    ----------\n    dataset_short_name : str\n        The dataset short name (era5-world etc).\n    variables : list[str]\n        List of variable names.\n    boundaries : list[float]\n        List of boundaries [north, west, south, east].\n\n    Returns\n    -------\n        str: A unique hash string representing the download parameters.\n    \"\"\"\n    # Create unique string from all parameters\n    param_string = f\"{dataset_short_name}|{sorted(variables)}|{boundaries}\"\n\n    # Generate hash\n    hash_object = hashlib.md5(param_string.encode())\n    return hash_object.hexdigest()[:12]  # 12 characters\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.file_management.find_existing_month_file","title":"find_existing_month_file","text":"<pre><code>find_existing_month_file(\n    save_dir, filename_base, year, month\n)\n</code></pre> <p>Tolerant matcher that finds an existing file for the given month. Accepts both <code>_YYYY-MM.ext</code> and <code>_YYYY_MM.ext</code> patterns and any extension.</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>Path</code> <p>Directory where files are saved.</p> required <code>filename_base</code> <code>str</code> <p>Base filename (without date or extension).</p> required <code>year</code> <code>int</code> <p>Year of the file.</p> required <code>month</code> <code>int</code> <p>Month of the file.</p> required <p>Returns:</p> Type Description <code>Optional[Path]</code> <p>Path to the existing file if found, else None.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/file_management.py</code> <pre><code>def find_existing_month_file(\n        save_dir: Path,\n        filename_base: str,\n        year: int,\n        month: int\n        ) -&gt; Optional[Path]:\n    \"\"\"\n    Tolerant matcher that finds an existing file for the given month.\n    Accepts both `_YYYY-MM.ext` and `_YYYY_MM.ext` patterns and any extension.\n\n    Parameters\n    ----------\n    save_dir : Path\n        Directory where files are saved.\n    filename_base : str\n        Base filename (without date or extension).\n    year : int\n        Year of the file.\n    month : int\n        Month of the file.\n\n    Returns\n    -------\n    Optional[Path]\n        Path to the existing file if found, else None.\n\n    \"\"\"\n    save_dir = Path(save_dir)\n    if not save_dir.exists():\n        return None\n\n    # Accept dash or underscore between year-month; any extension\n    pattern = re.compile(\n        rf\"^{re.escape(filename_base)}_(?:{year:04d}[-_]{month:02d})\\.[A-Za-z0-9]+$\"\n    )\n    for p in save_dir.iterdir():\n        if p.is_file() and pattern.match(p.name):\n            return p\n    return None\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.file_management.estimate_era5_monthly_file_size","title":"estimate_era5_monthly_file_size","text":"<pre><code>estimate_era5_monthly_file_size(\n    variables,\n    area,\n    grid_resolution=0.25,\n    timestep_hours=1.0,\n    bytes_per_value=4.0,\n)\n</code></pre> <p>Estimate ERA5 GRIB file size (MB) for a monthly dataset.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>list[str]</code> <p>Variables requested (e.g. ['2m_temperature', 'total_precipitation']).</p> required <code>area</code> <code>list[float]</code> <p>[north, west, south, east] geographic bounds in degrees.</p> required <code>grid_resolution</code> <code>float</code> <p>Grid spacing in degrees (default 0.25\u00b0 for ERA5).</p> <code>0.25</code> <code>timestep_hours</code> <code>float</code> <p>Temporal resolution in hours (1 = hourly, 3 = 3-hourly, 6 = 6-hourly, etc.).</p> <code>1.0</code> <code>bytes_per_value</code> <code>float</code> <p>Bytes per gridpoint per variable (float32 = 4 bytes).</p> <code>4.0</code> <p>Returns:</p> Type Description <code>float</code> <p>Estimated monthly file size in MB.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/file_management.py</code> <pre><code>def estimate_era5_monthly_file_size(\n    variables: list[str],\n    area: list[float],\n    grid_resolution: float = 0.25,\n    timestep_hours: float = 1.0,\n    bytes_per_value: float = 4.0,  # typically float32\n) -&gt; float:\n    \"\"\"\n    Estimate ERA5 GRIB file size (MB) for a monthly dataset.\n\n    Parameters\n    ----------\n    variables : list[str]\n        Variables requested (e.g. ['2m_temperature', 'total_precipitation']).\n    area : list[float]\n        [north, west, south, east] geographic bounds in degrees.\n    grid_resolution : float\n        Grid spacing in degrees (default 0.25\u00b0 for ERA5).\n    timestep_hours : float\n        Temporal resolution in hours (1 = hourly, 3 = 3-hourly, 6 = 6-hourly, etc.).\n    bytes_per_value : float\n        Bytes per gridpoint per variable (float32 = 4 bytes).\n\n    Returns\n    -------\n    float\n        Estimated monthly file size in MB.\n    \"\"\"\n    if not variables or not area:\n        return 0.0\n\n    north, west, south, east = area\n    n_vars = len(variables)\n\n    # --- Reference (from empirical files)\n    ref_size_MB = 0.509\n    ref_vars = 2\n    ref_area_deg2 = 3 * 2\n    ref_res = 0.25\n    ref_timestep_hours = 1.0\n    days_per_month = 30\n\n    # --- Compute derived quantities\n    # Handle wrap-around longitude\n    lon_span = (east - west) if east &gt; west else (360 + east - west)\n    lat_span = north - south\n    req_area_deg2 = lat_span * lon_span\n\n    # --- Compute scaling factors\n    area_factor = req_area_deg2 / ref_area_deg2\n    var_factor = n_vars / ref_vars\n    res_factor = (ref_res / grid_resolution) ** 2\n    timestep_factor = (ref_timestep_hours / timestep_hours)\n    time_factor = timestep_factor * (days_per_month / 30)  # normalize to 30 days\n\n    # --- Estimated size (MB)\n    estimated_MB = ref_size_MB * var_factor * area_factor * res_factor * time_factor\n\n    # --- Safety floor\n    return round(max(estimated_MB, 0.05), 3)  # at least 0.05 MB\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.file_management.estimate_cds_download","title":"estimate_cds_download","text":"<pre><code>estimate_cds_download(\n    variables,\n    area,\n    start_date,\n    end_date,\n    observed_speed_mbps,\n    grid_resolution=0.25,\n    timestep_hours=1.0,\n    bytes_per_value=4.0,\n    overhead_per_request_s=180.0,\n    overhead_per_var_s=12.0,\n)\n</code></pre> <p>Estimate per-file and total download size/time for CDS (ERA5) retrievals, using an empirically grounded file size model.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>list[str]</code> <p>Variables selected (e.g. ['2m_temperature', 'total_precipitation']).</p> required <code>area</code> <code>list[float]</code> <p>[north, west, south, east] bounds in degrees.</p> required <code>start_date</code> <code>str</code> <p>Date range (YYYY-MM-DD).</p> required <code>end_date</code> <code>str</code> <p>Date range (YYYY-MM-DD).</p> required <code>observed_speed_mbps</code> <code>float</code> <p>Measured internet speed in megabits per second (Mbps).</p> required <code>grid_resolution</code> <code>float</code> <p>Grid resolution in degrees (default 0.25\u00b0).</p> <code>0.25</code> <code>timestep_hours</code> <code>float</code> <p>Temporal resolution in hours (default 1-hourly).</p> <code>1.0</code> <code>bytes_per_value</code> <code>float</code> <p>Bytes per stored value (float32 = 4).</p> <code>4.0</code> <code>overhead_per_request_s</code> <code>float</code> <p>Fixed CDS request overhead time (queue/prep).</p> <code>180.0</code> <code>overhead_per_var_s</code> <code>float</code> <p>Per-variable overhead for CDS throttling/prep.</p> <code>12.0</code> <p>Returns:</p> Type Description <code>dict</code> <p>{   \"months\": int,   \"file_size_MB\": float,   \"total_size_MB\": float,   \"time_per_file_sec\": float,   \"total_time_sec\": float }</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/file_management.py</code> <pre><code>def estimate_cds_download(\n        variables: list[str],\n        area: list[float],\n        start_date: str,\n        end_date: str,\n        observed_speed_mbps: float,\n        grid_resolution: float = 0.25,\n        timestep_hours: float = 1.0,\n        bytes_per_value: float = 4.0,\n        overhead_per_request_s: float = 180.0,      # ignored but kept for signature consistency\n        overhead_per_var_s: float = 12.0,       # ignored but kept for signature consistency\n        ) -&gt; dict:\n    \"\"\"\n    Estimate per-file and total download size/time for CDS (ERA5) retrievals,\n    using an empirically grounded file size model.\n\n    Parameters\n    ----------\n    variables : list[str]\n        Variables selected (e.g. ['2m_temperature', 'total_precipitation']).\n    area : list[float]\n        [north, west, south, east] bounds in degrees.\n    start_date, end_date : str\n        Date range (YYYY-MM-DD).\n    observed_speed_mbps : float\n        Measured internet speed in megabits per second (Mbps).\n    grid_resolution : float, optional\n        Grid resolution in degrees (default 0.25\u00b0).\n    timestep_hours : float, optional\n        Temporal resolution in hours (default 1-hourly).\n    bytes_per_value : float, optional\n        Bytes per stored value (float32 = 4).\n    overhead_per_request_s : float, optional\n        Fixed CDS request overhead time (queue/prep).\n    overhead_per_var_s : float, optional\n        Per-variable overhead for CDS throttling/prep.\n\n    Returns\n    -------\n    dict\n        {\n          \"months\": int,\n          \"file_size_MB\": float,\n          \"total_size_MB\": float,\n          \"time_per_file_sec\": float,\n          \"total_time_sec\": float\n        }\n    \"\"\"\n    if not variables or not area or not start_date or not end_date:\n        return {\n            \"months\": 0,\n            \"file_size_MB\": 0.0,\n            \"total_size_MB\": 0.0,\n            \"time_per_file_sec\": 0.0,\n            \"total_time_sec\": 0.0,\n        }\n\n    # ---------- month list (inclusive) ----------\n    s = datetime.strptime(start_date, \"%Y-%m-%d\")\n    e = datetime.strptime(end_date, \"%Y-%m-%d\")\n    months = []\n    y, m = s.year, s.month\n    while (y &lt; e.year) or (y == e.year and m &lt;= e.month):\n        # days in this month\n        if m == 12:\n            next_first = datetime(y + 1, 1, 1)\n        else:\n            next_first = datetime(y, m + 1, 1)\n        this_first = datetime(y, m, 1)\n        dim = (next_first - this_first).days\n        months.append((y, m, dim))\n        if m == 12:\n            y, m = y + 1, 1\n        else:\n            m += 1\n\n    n_files = len(months)\n    n_vars = max(1, len(variables))\n\n    # ---------- area \u2192 grid cells ----------\n    north, west, south, east = map(float, area)\n    # handle wrap-around longitude\n    lon_span = (east - west) if east &gt; west else (360.0 + east - west)\n    lat_span = max(0.0, north - south)\n    lat_cells = max(1, int(math.ceil(lat_span / grid_resolution)))\n    lon_cells = max(1, int(math.ceil(lon_span / grid_resolution)))\n    grid_cells = lat_cells * lon_cells\n\n    # ---------- file sizes (use your existing estimator for MB) ----------\n    file_size_MB = estimate_era5_monthly_file_size(\n        variables=variables,\n        area=area,\n        grid_resolution=grid_resolution,\n        timestep_hours=timestep_hours,\n        bytes_per_value=bytes_per_value,\n    )\n    total_size_MB = file_size_MB * n_files\n\n    # ---------- timing model constants (tune if you like) ----------\n    BASELINE_SEC   = 30.0            # small per-request overhead\n    UNITS_PER_SEC  = 8_000_000.0     # processing throughput (units -&gt; seconds); lower = more conservative\n    SERVER_CAP_Mbps = 400.0          # upper network cap in megabits/sec (\u2248 50 MB/s)\n    SAFETY_FACTOR  = 5.0             # inflate to capture throttling, retries, etc.\n\n    # Effective MB/s (convert from Mbps \u2192 MB/s)\n    line_MBps = max(0.5, float(observed_speed_mbps) / 8.0)\n    srv_MBps  = max(0.5, SERVER_CAP_Mbps / 8.0)\n    eff_MBps  = min(line_MBps, srv_MBps)\n\n    per_file_secs = []\n    for (_, _, days_in_month) in months:\n        steps = int((24.0 / max(0.0001, timestep_hours)) * days_in_month)\n\n        # processing \u201cunits\u201d = grid_cells \u00d7 steps \u00d7 vars\n        units = grid_cells * steps * n_vars\n        processing_sec = BASELINE_SEC + (units / UNITS_PER_SEC)\n\n        # network time based on file size and effective MB/s\n        network_sec = file_size_MB / eff_MBps\n\n        per_file_secs.append((processing_sec + network_sec) * SAFETY_FACTOR)\n\n    time_per_file_sec = max(per_file_secs) if per_file_secs else 0.0\n    total_time_sec    = sum(per_file_secs)\n\n    return {\n        \"months\": n_files,\n        \"file_size_MB\": round(file_size_MB, 3),\n        \"total_size_MB\": round(total_size_MB, 3),\n        \"time_per_file_sec\": round(time_per_file_sec, 1),\n        \"total_time_sec\": round(number=total_time_sec, ndigits=1),\n    }\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.file_management.expected_save_path","title":"expected_save_path","text":"<pre><code>expected_save_path(\n    save_dir, filename_base, year, month, data_format=\"grib\"\n)\n</code></pre> <p>Construct canonical save path for monthly data.</p> <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>str | Path | None</code> <p>Base directory for saving. If None, defaults to osme_common.paths.data_dir().</p> required <code>filename_base</code> <code>str</code> <p>Base name without date or extension.</p> required <code>year</code> <code>int</code> <p>Year and month of the file.</p> required <code>month</code> <code>int</code> <p>Year and month of the file.</p> required <code>data_format</code> <code>str</code> <p>File extension, e.g., 'grib' or 'nc'.</p> <code>'grib'</code> <p>Returns:</p> Type Description <code>Path</code> <p>Resolved path under the proper data directory.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/file_management.py</code> <pre><code>def expected_save_path(\n        save_dir: str | Path | None,\n        filename_base: str,\n        year: int,\n        month: int,\n        data_format: str = \"grib\"\n        ) -&gt; Path:\n    \"\"\"\n    Construct canonical save path for monthly data.\n\n    Parameters\n    ----------\n    save_dir : str | Path | None\n        Base directory for saving. If None, defaults to osme_common.paths.data_dir().\n    filename_base : str\n        Base name without date or extension.\n    year, month : int\n        Year and month of the file.\n    data_format : str\n        File extension, e.g., 'grib' or 'nc'.\n\n    Returns\n    -------\n    Path\n        Resolved path under the proper data directory.\n    \"\"\"\n    base = resolve_under(data_dir(create=True), save_dir)\n    return base / f\"{filename_base}_{year:04d}-{month:02d}.{data_format}\"\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.logging","title":"weather_data_retrieval.utils.logging","text":""},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.logging.build_download_summary","title":"build_download_summary","text":"<pre><code>build_download_summary(session, estimates, speed_mbps)\n</code></pre> <p>Construct a formatted summary string of the current download configuration.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>Current session state containing all parameters.</p> required <code>estimates</code> <code>dict</code> <p>Dictionary containing download size and time estimates.</p> required <code>speed_mbps</code> <code>float</code> <p>Measured or estimated internet speed in Mbps.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Nicely formatted summary string for display or logging.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/logging.py</code> <pre><code>def build_download_summary(session: Any,\n                           estimates: dict,\n                           speed_mbps: float) -&gt; str:\n    \"\"\"\n    Construct a formatted summary string of the current download configuration.\n\n    Parameters\n    ----------\n    session : SessionState\n        Current session state containing all parameters.\n    estimates : dict\n        Dictionary containing download size and time estimates.\n    speed_mbps : float\n        Measured or estimated internet speed in Mbps.\n\n    Returns\n    -------\n    str\n        Nicely formatted summary string for display or logging.\n    \"\"\"\n    summary = (\n        f\"\\n\" + \"=\"* 60 +\n        f\"\\nDownload Request Summary\\n\" + \"=\"* 60 + \"\\n\"\n        f\"Provider: {session.get('data_provider').upper()}\\n\"\n        f\"Dataset: {session.get('dataset_short_name')}\\n\"\n        f\"Dates: {session.get('start_date')} \u2192 {session.get('end_date')}\\n\"\n        f\"Area: {session.get('region_bounds')}\\n\"\n        f\"Variables: {session.get('variables')}\\n\"\n        f\"Save Directory: {session.get('save_dir')}\\n\"\n        f\"Retries: {session.get('retry_settings')}\\n\"\n        f\"Parallelisation: {session.get('parallel_settings')}\\n\\n\"\n        f\"----------------------------------------\\n\\n\"\n        f\"Estimated number of monthly files: {estimates['months']}\\n\"\n        f\"Estimated size per file: {estimates['file_size_MB']:,.1f} MB\\n\"\n        f\"Estimated total size: {estimates['total_size_MB']:,.1f} MB\\n\\n\"\n        f\"Measured connection speed: {speed_mbps:.4f} Mbps\\n\\n\"\n        f\"Estimated maximum time per file: {_format_duration(estimates['time_per_file_sec'])}\\n\"\n        f\"Estimated maximum total time: {_format_duration(estimates['total_time_sec'])}\\n\"\n    )\n    return summary\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.logging.setup_logger","title":"setup_logger","text":"<pre><code>setup_logger(\n    save_dir=None, run_mode=\"interactive\", verbose=False\n)\n</code></pre> <p>Initialize and return a configured logger.</p> <p>Logs are written to /logs (or $OSME_LOG_DIR) by default, with optional console output in interactive or verbose modes. <p>Parameters:</p> Name Type Description Default <code>save_dir</code> <code>str or None</code> <p>Directory to save log files. If None, defaults to osme_common.paths.log_dir().</p> <code>None</code> <code>run_mode</code> <code>str</code> <p>Either 'interactive' or 'automatic', by default 'interactive'.</p> <code>'interactive'</code> <code>verbose</code> <code>bool</code> <p>Whether to echo logs to console in automatic mode, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Logger</code> <p>Configured logger instance.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/logging.py</code> <pre><code>def setup_logger(\n        save_dir: str | None = None,\n        run_mode: str = \"interactive\",\n        verbose: bool = False\n        ) -&gt; logging.Logger:\n    \"\"\"\n    Initialize and return a configured logger.\n\n    Logs are written to &lt;repo_root&gt;/logs (or $OSME_LOG_DIR) by default,\n    with optional console output in interactive or verbose modes.\n\n    Parameters\n    ----------\n    save_dir : str or None, optional\n        Directory to save log files. If None, defaults to osme_common.paths.log_dir().\n    run_mode : str, optional\n        Either 'interactive' or 'automatic', by default 'interactive'.\n    verbose : bool, optional\n        Whether to echo logs to console in automatic mode, by default False.\n\n    Returns\n    -------\n    logging.Logger\n        Configured logger instance.\n    \"\"\"\n    # --- Resolve log directory ---\n    base_dir = Path(save_dir) if save_dir else log_dir(create=True)\n    base_dir.mkdir(parents=True, exist_ok=True)\n\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_path = base_dir / f\"run_{run_mode}_{timestamp}.log\"\n\n    logger = logging.getLogger(\"weather_retrieval\")\n    # Make sure logger captures everything; handlers will filter\n    logger.setLevel(logging.DEBUG)\n\n    # Clear old handlers safely\n    if logger.hasHandlers():\n        for h in list(logger.handlers):\n            logger.removeHandler(h)\n\n    # File handler \u2014 DEBUG (captures prompts + everything)\n    fh = logging.FileHandler(log_path)\n    fh.setLevel(logging.DEBUG)\n    fh.setFormatter(logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\"))\n    logger.addHandler(fh)\n\n    # Console handler:\n    # - interactive: always show console at INFO+\n    # - automatic: show console only if verbose=True\n    add_console = (run_mode == \"interactive\") or (run_mode == \"automatic\" and verbose)\n    if add_console:\n        ch = logging.StreamHandler()\n        ch.setLevel(logging.INFO)   # &lt;- no DEBUG on console, so prompts won't duplicate\n        ch.setFormatter(logging.Formatter(\"%(message)s\"))\n        logger.addHandler(ch)\n\n    logger.info(f\"Logging initialized at {log_path}\")\n    return logger\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.logging.log_msg","title":"log_msg","text":"<pre><code>log_msg(msg, logger, *, level='info', echo_console=False)\n</code></pre> <p>Unified logging utility. - Always logs to file. - Echo to console (via tqdm.write) only in interactive mode.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>Message to log.</p> required <code>logger</code> <code>Logger</code> <p>Logger instance.</p> required <code>level</code> <code>str</code> <p>Logging level: 'info', 'warning', 'error', 'exception', by default \"info\".</p> <code>'info'</code> <code>echo_console</code> <code>bool</code> <p>Whether to also echo to console, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/logging.py</code> <pre><code>def log_msg(\n        msg: str,\n        logger,\n        *,\n        level: str = \"info\",\n        echo_console: bool = False\n        ) -&gt; None:\n    \"\"\"\n    Unified logging utility.\n    - Always logs to file.\n    - Echo to console (via tqdm.write) only in interactive mode.\n\n    Parameters\n    ----------\n    msg : str\n        Message to log.\n    logger : logging.Logger\n        Logger instance.\n    level : str, optional\n        Logging level: 'info', 'warning', 'error', 'exception', by default \"info\".\n    echo_console : bool, optional\n        Whether to also echo to console, by default False.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n    if not logger:\n        raise ValueError(\"Logger instance must be provided to log_msg().\")\n\n    log_fn = getattr(logger, level, logger.info)\n    log_fn(msg)\n\n    if echo_console:\n        tqdm.write(s=msg)\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.logging.create_final_log_file","title":"create_final_log_file","text":"<pre><code>create_final_log_file(\n    session,\n    filename_base,\n    original_logger,\n    *,\n    delete_original=True,\n    reattach_to_final=True\n)\n</code></pre> <p>Create a final log file with the same naming pattern as data files. Copies content from the original log file.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Any(SessionState)</code> <p>Current session state.</p> required <code>filename_base</code> <code>str</code> <p>Base filename pattern (same as data files).</p> required <code>original_logger</code> <code>Logger</code> <p>The original logger instance.</p> required <code>delete_original</code> <code>bool</code> <p>Whether to delete the original log file after creating the final one, by default True.</p> <code>True</code> <code>reattach_to_final</code> <code>bool</code> <p>Whether to reattach the logger to the final log file, by default True.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the final log file.</p> Source code in <code>packages/weather_data_retrieval/src/weather_data_retrieval/utils/logging.py</code> <pre><code>def create_final_log_file(\n        session,\n        filename_base: str,\n        original_logger: logging.Logger,\n        *,\n        delete_original: bool = True,\n        reattach_to_final: bool = True,\n        ) -&gt; str | None:\n    \"\"\"\n    Create a final log file with the same naming pattern as data files.\n    Copies content from the original log file.\n\n    Parameters\n    ----------\n    session : Any (SessionState)\n        Current session state.\n    filename_base : str\n        Base filename pattern (same as data files).\n    original_logger : logging.Logger\n        The original logger instance.\n    delete_original : bool, optional\n        Whether to delete the original log file after creating the final one, by default True.\n    reattach_to_final : bool, optional\n        Whether to reattach the logger to the final log file, by default True.\n\n    Returns\n    -------\n    str\n        Path to the final log file.\n    \"\"\"\n    # 1) locate the current FileHandler\n    fh = None\n    for h in original_logger.handlers:\n        if isinstance(h, logging.FileHandler):\n            fh = h\n            break\n\n    if fh is None or not hasattr(fh, \"baseFilename\"):\n        # No file handler found\n        return None\n\n    original_path = Path(fh.baseFilename)\n\n    # 2) build final path\n    save_dir_raw = session.get(\"save_dir\")\n    save_dir = resolve_under(data_dir(create=True), save_dir_raw) if save_dir_raw else data_dir(create=True)\n\n    start = session.get(\"start_date\")\n    end = session.get(\"end_date\")\n    retrieved = datetime.datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n\n    final_name = f\"{filename_base}_{start}-{end}_retrieved-{retrieved}.log\"\n    final_path = save_dir / final_name\n    final_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # 3) flush &amp; close the original handler, detach from logger\n    fh.flush()\n    fh.close()\n    original_logger.removeHandler(fh)\n\n    # 4) copy to final and optionally delete original\n    try:\n        # If same filesystem, you could also use os.replace to move instead of copy\n        shutil.copyfile(original_path, final_path)\n        if delete_original:\n            try:\n                os.remove(original_path)\n            except Exception:\n                pass\n    except Exception as e:\n        # Reattach the original handler if something failed so we don't lose logging\n        try:\n            fh = logging.FileHandler(original_path, encoding=\"utf-8\")\n            fh.setLevel(logging.DEBUG)\n            fh.setFormatter(logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\"))\n            original_logger.addHandler(fh)\n        except Exception:\n            pass\n        # Log to console if possible\n        original_logger.warning(f\"Failed to create final log file: {e}\")\n        return None\n\n    # 5) optionally attach a new FileHandler pointing at the final file\n    if reattach_to_final:\n        new_fh = logging.FileHandler(final_path, encoding=\"utf-8\")\n        new_fh.setLevel(logging.DEBUG)\n        new_fh.setFormatter(logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\"))\n        original_logger.addHandler(new_fh)\n\n    # 6) log a confirmation (now goes to final file if reattached, and to console in interactive mode)\n    try:\n        original_logger.info(f\"Final log file created: {final_path}\")\n    except Exception:\n        pass\n\n    return str(final_path)\n</code></pre>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.session_management","title":"weather_data_retrieval.utils.session_management","text":""},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.session_management.SessionState","title":"SessionState","text":""},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.session_management.SessionState.first_unfilled_key","title":"first_unfilled_key","text":"<pre><code>first_unfilled_key()\n</code></pre> <p>Return the first key in the ordered fields that is not filled. This enables a simple wizard-like progression and supports backtracking by clearing fields with <code>unset(key)</code>.</p>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.session_management.SessionState.summary","title":"summary","text":"<pre><code>summary()\n</code></pre> <p>Return a nice printable summary of all fields in a tabular format.</p>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.session_management.SessionState.to_dict","title":"to_dict","text":"<pre><code>to_dict(only_filled=False)\n</code></pre> <p>Flatten the session into a plain dict suitable for runner.run(...). If only_filled=True, include only keys that have been filled.</p> <p>Parameters:</p> Name Type Description Default <code>only_filled</code> <code>bool</code> <p>Whether to include only filled keys, by default False.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Flattened session dictionary.</p>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.session_management.get_cds_dataset_config","title":"get_cds_dataset_config","text":"<pre><code>get_cds_dataset_config(session, dataset_config_mapping)\n</code></pre> <p>Return dataset configuration dictionary based on session short name.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>SessionState</code> <p>The current session state containing user selections.</p> required <code>dataset_config_mapping</code> <code>dict</code> <p>The mapping of dataset short names to their configurations.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The configuration dictionary for the selected dataset.</p>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.session_management.map_config_to_session","title":"map_config_to_session","text":"<pre><code>map_config_to_session(cfg, session, *, logger=None)\n</code></pre> <p>Validate and map a loaded JSON config into SessionState.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>dict</code> <p>Loaded configuration dictionary.</p> required <code>session</code> <code>SessionState</code> <p>The session state to populate.</p> required Returns: <p>tuple : (bool, list[str])     (ok, messages): ok=False if any hard error prevents continuing.</p>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.session_management.ensure_cds_connection","title":"ensure_cds_connection","text":"<pre><code>ensure_cds_connection(\n    client,\n    creds,\n    max_reauth_attempts=6,\n    wait_between_attempts=15,\n)\n</code></pre> <p>Ensure a valid CDS API client. Re-authenticate automatically if the connection drops.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>Client</code> <p>Current CDS API client.</p> required <code>creds</code> <code>dict</code> <p>{'url': str, 'key': str} stored from initial login.</p> required <code>max_reauth_attempts</code> <code>int</code> <p>Maximum reconnection attempts before aborting.</p> <code>6</code> <code>wait_between_attempts</code> <code>int</code> <p>Wait time (seconds) between re-auth attempts.</p> <code>15</code> <p>Returns:</p> Type Description <code>Client | None</code> <p>Valid client or None if re-authentication ultimately fails.</p>"},{"location":"osme-weather-data-retrieval-documentation/codebase/#weather_data_retrieval.utils.session_management.internet_speedtest","title":"internet_speedtest","text":"<pre><code>internet_speedtest(\n    test_urls=None,\n    max_seconds=15,\n    logger=None,\n    echo_console=True,\n)\n</code></pre> <p>Download ~100MB test file from a fast CDN to estimate speed (MB/s).</p> <p>Parameters:</p> Name Type Description Default <code>test_urls</code> <code>list[str]</code> <p>List of URLs of the test files.</p> <code>None</code> <code>max_seconds</code> <code>int</code> <p>Maximum time to wait for a response, by default 15 seconds.</p> <code>15</code> <p>Returns:</p> Type Description <code>    float: Estimated download speed in Mbps.</code>"},{"location":"osme-weather-data-retrieval-documentation/quickstart/","title":"Quickstart","text":"<p>Currently under development!</p>"}]}